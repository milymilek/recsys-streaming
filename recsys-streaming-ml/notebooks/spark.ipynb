{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, struct, array\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, Union, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYSPARK_PYTHON=C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\Scripts\\python.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYSPARK_DRIVER_PYTHON=C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\Scripts\\python.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BROKER_URL = \"kafka0:9093\"\n",
    "RECOMMENDATIONS_TOPIC = \"recommendations\"\n",
    "USER_ACTIONS_TOPIC = \"users.actions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    # load model from checkpoint\n",
    "    import torch    \n",
    "    device = torch.device(\"cuda\")\n",
    "    model = Net().to(device)\n",
    "    checkpoint = load_checkpoint(checkpoint_dir)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    # define predict function in terms of numpy arrays\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        torch_inputs = torch.from_numpy(inputs).to(device)\n",
    "        outputs = model(torch_inputs)\n",
    "        return outputs.cpu().detach().numpy()\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"KafkaRead\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"user_id\", StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER_URL) \\\n",
    "    .option(\"subscribe\", RECOMMENDATIONS_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df = df.selectExpr(\"CAST(value AS STRING) as json_data\") \\\n",
    "                .select(from_json(col(\"json_data\"), schema).alias(\"data\")) \\\n",
    "                .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df.writeStream.format('console').outputMode('append').start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = values_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed = df.selectExpr(\"CAST(value AS STRING) as json_value\") \\\n",
    "    .select(from_json(col(\"json_value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_parsed.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.spark.utils import spark\n",
    "\n",
    "def create_dataframe_from_dict(spark, data):\n",
    "    \"\"\"\n",
    "    Create a DataFrame from a list of dictionaries.\n",
    "    Each dictionary represents a record with a single field `user_id`.\n",
    "    \"\"\"\n",
    "    schema = StructType([StructField(\"user_id\", StringType(), True)])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    return df\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    {\"user_id\": \"A1\"},\n",
    "    {\"user_id\": \"B2\"},\n",
    "    {\"user_id\": \"C3\"},\n",
    "    {\"user_id\": \"D4\"}\n",
    "]\n",
    "\n",
    "# Create Spark session\n",
    "session = spark()\n",
    "\n",
    "# Create DataFrame from data\n",
    "df = create_dataframe_from_dict(session, data)\n",
    "\n",
    "# Show DataFrame\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|     A1|\n",
      "|     B2|\n",
      "|     C3|\n",
      "|     D4|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "def create_item_feature_store(spark):\n",
    "    data = [(i, i) for i in range(100)]\n",
    "    schema = StructType([\n",
    "        StructField(\"parent_asin\", IntegerType(), True),\n",
    "        StructField(\"store_id\", IntegerType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    return df\n",
    "\n",
    "item_feature_store = create_item_feature_store(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|parent_asin|store_id|\n",
      "+-----------+--------+\n",
      "|          0|       0|\n",
      "|          1|       1|\n",
      "|          2|       2|\n",
      "|          3|       3|\n",
      "|          4|       4|\n",
      "|          5|       5|\n",
      "|          6|       6|\n",
      "|          7|       7|\n",
      "|          8|       8|\n",
      "|          9|       9|\n",
      "|         10|      10|\n",
      "|         11|      11|\n",
      "|         12|      12|\n",
      "|         13|      13|\n",
      "|         14|      14|\n",
      "|         15|      15|\n",
      "|         16|      16|\n",
      "|         17|      17|\n",
      "|         18|      18|\n",
      "|         19|      19|\n",
      "+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_feature_store.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MONGO] Connection successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\pandas\\__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from recsys_streaming_ml.db import mongo_db, read_df_from_mongo\n",
    "from recsys_streaming_ml.data.utils import load_feature_maps\n",
    "import random\n",
    "\n",
    "feature_maps = load_feature_maps(\"../.data/feature_maps.pkl\")\n",
    "\n",
    "def read_item_feature_store(db, feature_maps, collection='metadata'):\n",
    "    item_feature_store_raw = read_df_from_mongo(db=db, collection=collection)\n",
    "    item_feature_store = item_feature_store_raw.copy()\n",
    "    item_feature_store['parent_asin'] = item_feature_store['parent_asin'].map(feature_maps['parent_id_map'])\n",
    "    item_feature_store['store_id'] = item_feature_store['store'].map(feature_maps['store_id_map'])\n",
    "    item_feature_store = item_feature_store.drop(columns='store').dropna().astype(int).sort_values(by='parent_asin').reset_index(drop=True)\n",
    "\n",
    "    return item_feature_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_store = read_item_feature_store(mongo_db, feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_store = session.createDataFrame(item_feature_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|parent_asin|store_id|\n",
      "+-----------+--------+\n",
      "|          0|      46|\n",
      "|          1|      74|\n",
      "|          2|      47|\n",
      "|          3|     209|\n",
      "|          4|      48|\n",
      "|          5|     238|\n",
      "|          6|     179|\n",
      "|          7|     239|\n",
      "|          8|      49|\n",
      "|          9|      75|\n",
      "|         10|     132|\n",
      "|         11|     111|\n",
      "|         12|      18|\n",
      "|         13|     112|\n",
      "|         14|       0|\n",
      "|         15|      19|\n",
      "|         16|       1|\n",
      "|         17|     113|\n",
      "|         18|       0|\n",
      "|         19|     240|\n",
      "+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_feature_store.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, create_map, lit, udf\n",
    "from itertools import chain\n",
    "\n",
    "user_id_mapping = {\"A1\": 2, \"B2\": 3, \"C3\": 1, \"D4\": 0}\n",
    "rev_user_id_mapping = {v:k for k,v in user_id_mapping.items()}\n",
    "rev_asin_mapping = {v[0]:f'ID_{v[0]}' for v in item_feature_store.select('parent_asin').distinct().collect()}\n",
    "\n",
    "def process_data(\n",
    "        df: ps.sql.dataframe.DataFrame, \n",
    "        item_feature_store: ps.sql.dataframe.DataFrame, \n",
    "        user_id_mapping: dict[str, int]\n",
    "    ) -> ps.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the DataFrame by mapping user_ids using the provided dictionary.\n",
    "    \"\"\"\n",
    "    mapping_expr = create_map([lit(x) for x in chain(*user_id_mapping.items())])\n",
    "\n",
    "    processed_df = df.withColumn(\"map_user_id\", mapping_expr[col(\"user_id\")])\n",
    "    processed_df = processed_df.crossJoin(item_feature_store)\n",
    "    processed_df = processed_df.select(\"map_user_id\", \"parent_asin\", \"store_id\")\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = process_data(df, item_feature_store, user_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------+\n",
      "|map_user_id|parent_asin|store_id|\n",
      "+-----------+-----------+--------+\n",
      "|          2|          0|       0|\n",
      "|          2|          1|       1|\n",
      "|          2|          2|       2|\n",
      "|          2|          3|       3|\n",
      "|          2|          4|       4|\n",
      "|          2|          5|       5|\n",
      "|          2|          6|       6|\n",
      "|          2|          7|       7|\n",
      "|          2|          8|       8|\n",
      "|          2|          9|       9|\n",
      "|          2|         10|      10|\n",
      "|          2|         11|      11|\n",
      "|          2|         12|      12|\n",
      "|          2|         13|      13|\n",
      "|          2|         14|      14|\n",
      "|          2|         15|      15|\n",
      "|          2|         16|      16|\n",
      "|          2|         17|      17|\n",
      "|          2|         18|      18|\n",
      "|          2|         19|      19|\n",
      "+-----------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, collect_list\n",
    "\n",
    "def get_ranked_topk_predictions(df, k=5):\n",
    "    window = Window.partitionBy(\"map_user_id\").orderBy(col(\"predicted_rating\").desc())\n",
    "\n",
    "    # Add a rank column to rank the rows within each partition by 'sum'\n",
    "    ranked_predictions = df.withColumn(\"rank\", rank().over(window))\n",
    "\n",
    "    # Filter to keep only the top 5 'asin' values for each 'map_user_id'\n",
    "    top_k = ranked_predictions.filter(col(\"rank\") <= k)\n",
    "\n",
    "    return top_k\n",
    "\n",
    "def remap_entities(df, user_id_mapping, asin_mapping):\n",
    "        mapping_expr_user = create_map([lit(x) for x in chain(*user_id_mapping.items())])\n",
    "        mapping_expr_asin = create_map([lit(x) for x in chain(*asin_mapping.items())])\n",
    "\n",
    "        df = df.withColumn(\"user_id\", mapping_expr_user[col(\"map_user_id\")])\n",
    "        df = df.withColumn(\"asin\", mapping_expr_asin[col(\"map_user_id\")])\n",
    "\n",
    "        return df.select(\"user_id\", \"asin\", \"rank\")\n",
    "\n",
    "def list_recommendations(df):\n",
    "    # Aggregate the top k 'asin' values into a list for each 'map_user_id'\n",
    "    result = df.groupBy(\"user_id\").agg(collect_list(\"asin\").alias(\"top_k_asins\"))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_topk = get_ranked_topk_predictions(predictions)\n",
    "remapped_ranked_topk = remap_entities(ranked_topk, rev_user_id_mapping, rev_asin_mapping)\n",
    "recommendation_lists = list_recommendations(remapped_ranked_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_topk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remapped_ranked_topk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_lists.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/path/to/test/data\")\n",
    "preds = df.withColumn(\"preds\", mnist('data')).collect()\n",
    "\n",
    "query = df_parsed.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    #.trigger(processingTime='15 seconds') \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../.data/dataset/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.data.utils import load_feature_maps\n",
    "import random\n",
    "\n",
    "feature_maps = load_feature_maps(\"../.data/feature_maps.pkl\")\n",
    "\n",
    "random_user_ids = random.choices(list(feature_maps['user_id_map'].keys()), k=50)\n",
    "pd.DataFrame(random_user_ids, columns=['user_id']).to_csv(\"../.data/sample_user_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "sys.path.append(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DISABLE_SPARK_ENVS'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.spark.utils import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys-streaming-ml\\.data\\dataset\\dataset_spark.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:/Users/Milosz/Projects/recsys-streaming/recsys-streaming-ml/.data/dataset/dataset_spark.csv/part-00000-b5b5d215-6187-49ea-905e-70202bd20248-c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = session.read.option(\"header\", \"true\").csv(\"C:/Users/Milosz/Projects/recsys-streaming/recsys-streaming-ml/.data/dataset/dataset_spark.csv/part-00000-b5b5d215-6187-49ea-905e-70202bd20248-c000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o57.csv.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:180)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:133)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:68)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:539)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:405)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/Milosz/Projects/recsys-streaming/recsys-streaming-ml/.data/dataset/dataset_spark.csv/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o57.csv.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:180)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:133)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:68)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:539)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:405)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n"
     ]
    }
   ],
   "source": [
    "spark_df = session.read.option(\"header\", \"true\").csv(\"C:/Users/Milosz/Projects/recsys-streaming/recsys-streaming-ml/.data/dataset/dataset_spark.csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../.data/dataset'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"../\" + DATASET_FILE.as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.config import DATASET_FILE\n",
    "import os\n",
    "def find_csv_file(directory_path):\n",
    "    # Iterate over the files in the directory\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            # Check if the file has a .csv suffix\n",
    "            if file.endswith('.csv'):\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(find_csv_file(\"../\" + DATASET_FILE.as_posix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.data.utils import load_feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = load_feature_maps('../.data/feature_maps.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['user_id_map', 'parent_id_map', 'store_id_map'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8029      0\n",
       "8030      0\n",
       "8031      0\n",
       "8032      0\n",
       "8028      0\n",
       "       ... \n",
       "9355    376\n",
       "9354    376\n",
       "9353    376\n",
       "9359    376\n",
       "9335    376\n",
       "Name: store, Length: 16201, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.store.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Crunchy Mama Box': 0,\n",
       " 'Mechaly': 1,\n",
       " 'CLUBTAC': 2,\n",
       " 'The Dapper Dog Box': 3,\n",
       " 'Coffee Beanery': 4,\n",
       " 'Transcends Monthly Crystal Subscription': 5,\n",
       " 'Mysteries in Time - adventures through history': 6,\n",
       " 'Pusheen': 7,\n",
       " 'Bitsbox': 8,\n",
       " 'La Fraise Boutique Adalie': 9,\n",
       " 'Beauteque': 10,\n",
       " 'meowbox': 11,\n",
       " 'MadrasIn': 12,\n",
       " \"Well Read's Box of Books\": 13,\n",
       " 'MindfulSouls': 14,\n",
       " 'VAHDAM': 15,\n",
       " 'Geek Fuel EXP': 16,\n",
       " 'Tinkering Toddler Crates': 17,\n",
       " 'Autsy Box': 18,\n",
       " 'Dayspa Body Basics': 19,\n",
       " 'Bath Blessing Box': 20,\n",
       " 'Avelas': 21,\n",
       " 'RICK AND MORTY': 22,\n",
       " 'SHIMAJIRO': 23,\n",
       " 'Nature Gnaws': 24,\n",
       " 'My Coffee and Book Club': 25,\n",
       " 'KaffeeScape': 26,\n",
       " 'goScribbler': 27,\n",
       " 'Boujee Brown': 28,\n",
       " 'Skivvie NIX': 29,\n",
       " 'LiveGlam Inc.': 30,\n",
       " 'Pressed': 31,\n",
       " 'SOCK FANCY': 32,\n",
       " 'Inked, LLC': 33,\n",
       " 'Snoqualmie Falls Candy Shoppe': 34,\n",
       " 'Enchanted Crystal': 35,\n",
       " 'Loved Again Media': 36,\n",
       " 'GetFaithbox': 37,\n",
       " 'Chakra Box': 38,\n",
       " 'Aroma Thyme': 39,\n",
       " 'Trendy Apparel Shop': 40,\n",
       " 'Loot Crate, Inc.': 41,\n",
       " 'Driftaway Coffee': 42,\n",
       " 'Frito Lay': 43,\n",
       " 'Foot Cardigan': 44,\n",
       " 'PupJoy': 45,\n",
       " 'Altman Plants': 46,\n",
       " 'My First Reading Club': 47,\n",
       " 'Sock Panda': 48,\n",
       " 'Bubbles&Books': 49,\n",
       " \"America's Test Kitchen Kids\": 50,\n",
       " 'Rays of Light': 51,\n",
       " \"ERIKA'S TEA ROOM MEMORIES MADE WITH EVERY CUP!\": 52,\n",
       " 'River Colony Trading': 53,\n",
       " 'Smartass & Sass': 54,\n",
       " 'Animal Jam Box': 55,\n",
       " 'Cross Box Kits': 56,\n",
       " 'Vnus Box': 57,\n",
       " 'Star Trek': 58,\n",
       " 'Pub Shirt Club': 59,\n",
       " 'Arctify Marvel': 60,\n",
       " 'Primal Kitchen': 61,\n",
       " 'Jeep': 62,\n",
       " 'PupMomCrate': 63,\n",
       " 'EmbraceBox': 64,\n",
       " 'Groovy Lab in a Box': 65,\n",
       " 'BOLDBOX': 66,\n",
       " 'STAR WARS': 67,\n",
       " 'Fat Brain Toys': 68,\n",
       " 'SoKawaii': 69,\n",
       " 'Urban You for Me': 70,\n",
       " 'Love Aiki': 71,\n",
       " 'Buffalo Bills': 72,\n",
       " 'SnackSack': 73,\n",
       " 'South Park': 74,\n",
       " 'Confetti Dash': 75,\n",
       " 'Play Learn Repeat': 76,\n",
       " 'Guitar Crate': 77,\n",
       " 'Puzzle Culture': 78,\n",
       " 'Watch Gang': 79,\n",
       " 'SockFly': 80,\n",
       " 'Hippie Dippie': 81,\n",
       " 'PriceMeNow': 82,\n",
       " 'Reveal Book Box': 83,\n",
       " 'Asurion': 84,\n",
       " 'Bombs and Bubbles Box': 85,\n",
       " 'The GEMS Camp': 86,\n",
       " 'Ignite English': 87,\n",
       " 'Avatar The Last Airbender': 88,\n",
       " 'Makeblock': 89,\n",
       " 'Kevia Style, LLC': 90,\n",
       " 'PHILOSOCKPHY': 91,\n",
       " 'Little Red Kitchen Bake Shop': 92,\n",
       " 'Just Glow': 93,\n",
       " 'Tea Runners': 94,\n",
       " 'Supernow': 95,\n",
       " 'Barbie': 96,\n",
       " 'Lash Stuff Simply Exquisite': 97,\n",
       " 'Nickelodeon': 98,\n",
       " 'Our Aroma': 99,\n",
       " 'Cotton Cuts LLC LIMITED LIABILITY COMPANY MISSOURI': 100,\n",
       " 'Brick Loot': 101,\n",
       " 'MFRC Wish Box': 102,\n",
       " 'Lovers Fresh Baked': 103,\n",
       " 'Fantasy Monthly': 104,\n",
       " 'The Expanse': 105,\n",
       " 'Stranger Things': 106,\n",
       " 'SnackNation': 107,\n",
       " 'Snoopy': 108,\n",
       " 'Love Goodly': 109,\n",
       " 'Green Kid Crafts': 110,\n",
       " 'KNITCRATE': 111,\n",
       " 'teawrks': 112,\n",
       " 'The Ring Boxes': 113,\n",
       " 'Inspire Me Korea Ltd': 114,\n",
       " 'Irish at Heart': 115,\n",
       " 'The Comic Garage': 116,\n",
       " 'eat2explore': 117,\n",
       " 'The FitBoxx': 118,\n",
       " 'Juice From the RAW': 119,\n",
       " 'The Legend of Vox Machina': 120,\n",
       " 'Hot Wheels': 121,\n",
       " 'My Thrill Club': 122,\n",
       " 'Psychic envy': 123,\n",
       " 'STEM Club': 124,\n",
       " 'Grateful Bread Company LLC': 125,\n",
       " 'LOLJerky': 126,\n",
       " 'Coastal Crafts DIY': 127,\n",
       " 'JoJo Siwa': 128,\n",
       " 'Zero Pack': 129,\n",
       " 'Holstee': 130,\n",
       " 'Simple Loose Leaf Tea Company': 131,\n",
       " 'Costa Farms': 132,\n",
       " 'X Workbox': 133,\n",
       " 'The Paisley Box': 134,\n",
       " \"Annie's\": 135,\n",
       " 'Comic Book Subs': 136,\n",
       " 'PINK PEWTER': 137,\n",
       " '123 BABY BOX': 138,\n",
       " 'Amazon Music': 139,\n",
       " 'OT For Me': 140,\n",
       " 'Boost Your BBQ': 141,\n",
       " 'Drag Society': 142,\n",
       " 'GQ Best Stuff': 143,\n",
       " 'Space & Beyond Box': 144,\n",
       " 'SONAGE': 145,\n",
       " 'Fullscale Subscription Box': 146,\n",
       " 'THE FABULOUS PLANNER': 147,\n",
       " 'Smalltown Table': 148,\n",
       " 'WWE': 149,\n",
       " 'Muse Illuminate Store': 150,\n",
       " 'Loot Crate': 151,\n",
       " 'SpiceBreeze': 152,\n",
       " 'Allure Beauty Box': 153,\n",
       " 'Jurassic World': 154,\n",
       " '50 States Of Mine': 155,\n",
       " 'Elevate Snack Box': 156,\n",
       " 'Succulent Studios': 157,\n",
       " 'Mainstreet Flower Market': 158,\n",
       " 'TinkerTots Boxes': 159,\n",
       " 'Chasin Unicorns': 160,\n",
       " 'Neverland Books': 161,\n",
       " 'PPC Store': 162,\n",
       " 'GlobeIn': 163,\n",
       " 'Match Made Coffee': 164,\n",
       " 'KiwiGrub': 165,\n",
       " 'Silverquicken': 166,\n",
       " 'Cairn': 167,\n",
       " 'Tomorrows Laundry': 168,\n",
       " 'Speach Family Candy Shoppe, Inc': 169,\n",
       " 'COCOTIQUE': 170,\n",
       " 'Clio Coffee Inc': 171,\n",
       " 'SPREZZA': 172,\n",
       " 'Southern Skeins Yarn': 173,\n",
       " 'STEM Discovery Boxes': 174,\n",
       " 'amazon music': 175,\n",
       " 'Cooper & Kid': 176,\n",
       " 'Dungeons & Dragons': 177,\n",
       " 'BloomsyBox': 178,\n",
       " 'BoxDog': 179,\n",
       " 'Highlights for Children': 180,\n",
       " 'The Kids Craft': 181,\n",
       " 'Czech Beads Exclusive': 182,\n",
       " 'Field to Cup': 183,\n",
       " 'Mini Mitts': 184,\n",
       " 'The Office': 185,\n",
       " 'My Hero Crate': 186,\n",
       " 'Wonder Crate': 187,\n",
       " 'How to Be A Redhead': 188,\n",
       " 'No-Gi Surprise': 189,\n",
       " 'CratedWithLove': 190,\n",
       " 'Artshine Inc': 191,\n",
       " 'Downtown Pet Supply': 192,\n",
       " 'The Russia Box': 193,\n",
       " 'Active Kids': 194,\n",
       " 'The Care Crate Co.': 195,\n",
       " 'Carnivore Club': 196,\n",
       " 'SPONGEBOB SQUAREPANTS': 197,\n",
       " 'Gogh Box Art Crate': 198,\n",
       " 'Le Panier Francais': 199,\n",
       " 'Adults & Crafts': 200,\n",
       " 'The Herb Collective': 201,\n",
       " 'ChaskiBox': 202,\n",
       " 'Jerky Subscription': 203,\n",
       " 'KIDS READ DAILY': 204,\n",
       " 'Funko': 205,\n",
       " 'Raddish': 206,\n",
       " 'Lady Box': 207,\n",
       " 'ESSENCE SKINCARE': 208,\n",
       " 'Skylar': 209,\n",
       " 'Bean Box': 210,\n",
       " 'Perfect Samplers': 211,\n",
       " 'The Dally Grind': 212,\n",
       " 'iBbeautiful': 213,\n",
       " 'Crush Crate': 214,\n",
       " 'DreamCutFlowers': 215,\n",
       " 'ELLA Inspires Magazine': 216,\n",
       " 'The Storytime Box': 217,\n",
       " 'Foodbevy': 218,\n",
       " 'BATTLBOX': 219,\n",
       " 'Preschool2U': 220,\n",
       " 'Hunt A Killer': 221,\n",
       " 'The Pink Sugar Box': 222,\n",
       " 'Rael, Inc': 223,\n",
       " 'hotspot pets': 224,\n",
       " 'Snack Mountain': 225,\n",
       " 'Basic Man': 226,\n",
       " 'Marvel': 227,\n",
       " 'Quilty Box': 228,\n",
       " 'Pawstruck': 229,\n",
       " 'E Squared Sweets LLC': 230,\n",
       " 'La Tea Dah': 231,\n",
       " 'MunchPak': 232,\n",
       " 'Feeling Fab Box': 233,\n",
       " 'SUCCULENTS BOX': 234,\n",
       " 'Gainz Box': 235,\n",
       " 'Korean Snack Box': 236,\n",
       " 'YVO - Your Very Own': 237,\n",
       " 'TheraBox': 238,\n",
       " \"Nelson's Tea\": 239,\n",
       " 'Wag Healthy Club': 240,\n",
       " 'Little Dreamers Club': 241,\n",
       " 'Disney': 242,\n",
       " 'LUXSB': 243,\n",
       " 'My Toothbrush Club': 244,\n",
       " 'Sovereign Code': 245,\n",
       " 'CatLadyBox': 246,\n",
       " 'RescueBox': 247,\n",
       " 'Think Outside Boxes': 248,\n",
       " 'Jerky Snob': 249,\n",
       " 'Miraculous': 250,\n",
       " 'Beauty By Francesca': 251,\n",
       " 'BabyFaceDiary': 252,\n",
       " 'BUNNY · JAMES .': 253,\n",
       " 'DateBox Club': 254,\n",
       " 'Eyescream Beauty': 255,\n",
       " 'Wickbox': 256,\n",
       " 'Your Fam Box': 257,\n",
       " 'L.O.L. Surprise!': 258,\n",
       " \"Rollin' n Bowlin'\": 259,\n",
       " 'Kidstir': 260,\n",
       " 'Cajun Crate': 261,\n",
       " 'We Craft Box': 262,\n",
       " 'LOONEY TUNES': 263,\n",
       " 'DC Comics': 264,\n",
       " 'Tea Sparrow': 265,\n",
       " 'Date Night In Box': 266,\n",
       " 'Dose of Literature': 267,\n",
       " 'IScape Candle Co.': 268,\n",
       " 'HARRY POTTER': 269,\n",
       " 'Mel': 270,\n",
       " 'KOS Coffee': 271,\n",
       " 'Toy Box Monthly': 272,\n",
       " 'My Christmas Crate': 273,\n",
       " 'Bokksu': 274,\n",
       " \"Medusa's Make-Up\": 275,\n",
       " 'Baketivity': 276,\n",
       " 'CultureFly': 277,\n",
       " 'SANRIO': 278,\n",
       " 'Crystalized Sparkle Box': 279,\n",
       " 'iMagicNails': 280,\n",
       " 'Magic The Gathering': 281,\n",
       " 'Home Made Luxe Craft Kits': 282,\n",
       " 'Platterful': 283,\n",
       " 'Madrasin': 284,\n",
       " 'breobox': 285,\n",
       " 'JAPANCRATE': 286,\n",
       " 'MMIZOO': 287,\n",
       " 'ArtSnacks': 288,\n",
       " 'The Keto Box': 289,\n",
       " 'Dungeon in a Box': 290,\n",
       " 'ATLG': 291,\n",
       " 'Kids American Monthly Subscription Box': 292,\n",
       " 'Mullybox': 293,\n",
       " 'Coloring And Classics': 294,\n",
       " 'Vegancuts': 295,\n",
       " 'MTV': 296,\n",
       " 'Junior Chef Box': 297,\n",
       " 'Fit Snack Box': 298,\n",
       " 'Sips By': 299,\n",
       " 'Tribe Beauty Box': 300,\n",
       " 'Masters of the Universe: Revelation': 301,\n",
       " 'OHM Beads': 302,\n",
       " 'bravbox': 303,\n",
       " 'June Moon Spice Company': 304,\n",
       " 'For The Faithful': 305,\n",
       " 'Bold Box': 306,\n",
       " 'Trendy Butler': 307,\n",
       " 'ParaBox Monthly': 308,\n",
       " 'Comicrealm': 309,\n",
       " 'Seoul Box': 310,\n",
       " 'MALEBASICS': 311,\n",
       " 'Barkbox': 312,\n",
       " 'FACETORY': 313,\n",
       " 'Ortega': 314,\n",
       " 'Learn With Mochi': 315,\n",
       " 'EnjoyLeggings': 316,\n",
       " 'Treats': 317,\n",
       " 'The Beatles': 318,\n",
       " 'Orgain': 319,\n",
       " 'The Winston Box': 320,\n",
       " 'Lip Monthly': 321,\n",
       " 'The Beard Club': 322,\n",
       " 'Disney Pixar': 323,\n",
       " 'OjO Games and Toys': 324,\n",
       " 'Simply Created': 325,\n",
       " 'Culture Fly': 326,\n",
       " 'Bravado Music Icons': 327,\n",
       " 'DAYFEN': 328,\n",
       " 'Sprezza': 329,\n",
       " 'Tipsy Pixie Tees': 330,\n",
       " 'Paperkitty': 331,\n",
       " 'Candy Club': 332,\n",
       " 'DolliBu': 333,\n",
       " 'Classic Candy Box': 334,\n",
       " 'BARKBOX': 335,\n",
       " 'Naruto Shippuden': 336,\n",
       " 'Flaire': 337,\n",
       " 'POUTBOX': 338,\n",
       " 'Cotton Cuts': 339,\n",
       " 'BARK IF YOU WANT SOME': 340,\n",
       " 'ColombianCoffeeUS': 341,\n",
       " \"Annie's Crafts\": 342,\n",
       " 'badboxx': 343,\n",
       " 'FetchBox': 344,\n",
       " 'Explore By Mail': 345,\n",
       " 'Toynk': 346,\n",
       " 'Herb + Stone Apothecary': 347,\n",
       " 'American Plant Exchange': 348,\n",
       " 'Cora Crea Crafts': 349,\n",
       " 'TROVE': 350,\n",
       " 'Prime Accessories HQ': 351,\n",
       " 'Enjoy Flowers': 352,\n",
       " 'Hamptons Box Club': 353,\n",
       " 'KIWI ECO BOX': 354,\n",
       " 'Cosmopolitan': 355,\n",
       " 'Little Passports': 356,\n",
       " 'Heavenly Candle': 357,\n",
       " 'Sandvik Publishing': 358,\n",
       " 'EXCELIUS': 359,\n",
       " 'KitNipBox': 360,\n",
       " 'Essence Of Argan': 361,\n",
       " 'noblo': 362,\n",
       " 'JARMABOX': 363,\n",
       " 'LivingBetter50': 364,\n",
       " 'Sticker Savages': 365,\n",
       " 'Hawthorne House': 366,\n",
       " 'The Killer Coffee Co': 367,\n",
       " 'TOKYOTREAT': 368,\n",
       " 'Tonies': 369,\n",
       " 'TeaFavors': 370,\n",
       " 'IntrovertsRetreat': 371,\n",
       " 'Fearless Nail Art': 372,\n",
       " 'Akibento': 373,\n",
       " 'Pokémon': 374,\n",
       " 'STRONG self(ie)': 375,\n",
       " \"Gentleman's Box\": 376}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps['store_id_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        264\n",
       "1        264\n",
       "2        264\n",
       "3        264\n",
       "4        264\n",
       "        ... \n",
       "16196     44\n",
       "16197     44\n",
       "16198     44\n",
       "16199    102\n",
       "16200    264\n",
       "Name: store, Length: 16201, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+-------+-----+\n",
      "|parent_asin|    timestamp|rating|user_id|store|\n",
      "+-----------+-------------+------+-------+-----+\n",
      "|          0|1639408671038|   2.0|   6551|  264|\n",
      "|          0|1670094428392|   5.0|   3191|  264|\n",
      "|          0|1653138455274|   5.0|  14957|  264|\n",
      "|          0|1636644871638|   5.0|   8711|  264|\n",
      "|          0|1639006187872|   5.0|   7138|  264|\n",
      "|          0|1652340580030|   3.0|  12961|  264|\n",
      "|          0|1633974025922|   5.0|   9479|  264|\n",
      "|          0|1649532524800|   5.0|      0|  264|\n",
      "|          0|1635107916926|   3.0|   3433|  264|\n",
      "|          0|1634350509767|   5.0|   6782|  264|\n",
      "|          0|1635187736470|   4.0|    298|  264|\n",
      "|          0|1633827953703|   5.0|   2841|  264|\n",
      "|          0|1666670589458|   5.0|  14892|  264|\n",
      "|          0|1648242079936|   5.0|   1184|  264|\n",
      "|          0|1640290884841|   5.0|  10483|  264|\n",
      "|          0|1633800053883|   5.0|   3583|  264|\n",
      "|          1|1565731866942|   1.0|  14233|   10|\n",
      "|          1|1594956895069|   5.0|  11200|   10|\n",
      "|          1|1624322553032|   4.0|  11697|   10|\n",
      "|          1|1602677834804|   5.0|   2696|   10|\n",
      "+-----------+-------------+------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.db import read_df_from_mongo, mongo_db\n",
    "df_ratings_historical = read_df_from_mongo(db=mongo_db, collection='ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rating</th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1602133857705</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AEMJ2EG5ODOCYUTI54NBXZHDJGSQ</td>\n",
       "      <td>B09WC47S3V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1609110735433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>AEEJBFZKUBEEMBZUZJV4UHFVEEBQ</td>\n",
       "      <td>B07QL1JRCN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1609937315319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AGSVZNZBTSGQBKZDZTQYEZHGDPCQ</td>\n",
       "      <td>B08N5QKX1Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1616156351887</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AFDERNB6BIR3U2DOR3S2KX7KJJXQ</td>\n",
       "      <td>B07KM6T8GV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1559533206066</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AE6P2YJ6FKX332MD56GPJFSHXNJQ</td>\n",
       "      <td>B07NVKNVNM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16211</th>\n",
       "      <td>1608846627043</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AGX4UTCIP4SGTHHDA4E3WGPIJFBQ</td>\n",
       "      <td>B09TMNNYWH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16212</th>\n",
       "      <td>1552764727421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AH4WBYSLGIAMVLCT2PPRTMNNJD6A</td>\n",
       "      <td>B07GNX34XV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16213</th>\n",
       "      <td>1575580196049</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AHUTNTBX5E65YNIHI2ERA2AMFM4Q</td>\n",
       "      <td>B07VXY755K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16214</th>\n",
       "      <td>1656510163413</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AF6RXEUYBSSOMO5RSS23OPB5QKSA</td>\n",
       "      <td>B07VXY755K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16215</th>\n",
       "      <td>1626557904080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AHNDJGN3XW2IH2HQFTNSNOKB6NSQ</td>\n",
       "      <td>B07VXY755K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16216 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           timestamp  rating                       user_id parent_asin\n",
       "0      1602133857705     1.0  AEMJ2EG5ODOCYUTI54NBXZHDJGSQ  B09WC47S3V\n",
       "1      1609110735433     2.0  AEEJBFZKUBEEMBZUZJV4UHFVEEBQ  B07QL1JRCN\n",
       "2      1609937315319     1.0  AGSVZNZBTSGQBKZDZTQYEZHGDPCQ  B08N5QKX1Y\n",
       "3      1616156351887     5.0  AFDERNB6BIR3U2DOR3S2KX7KJJXQ  B07KM6T8GV\n",
       "4      1559533206066     5.0  AE6P2YJ6FKX332MD56GPJFSHXNJQ  B07NVKNVNM\n",
       "...              ...     ...                           ...         ...\n",
       "16211  1608846627043     5.0  AGX4UTCIP4SGTHHDA4E3WGPIJFBQ  B09TMNNYWH\n",
       "16212  1552764727421     1.0  AH4WBYSLGIAMVLCT2PPRTMNNJD6A  B07GNX34XV\n",
       "16213  1575580196049     5.0  AHUTNTBX5E65YNIHI2ERA2AMFM4Q  B07VXY755K\n",
       "16214  1656510163413     1.0  AF6RXEUYBSSOMO5RSS23OPB5QKSA  B07VXY755K\n",
       "16215  1626557904080     1.0  AHNDJGN3XW2IH2HQFTNSNOKB6NSQ  B07VXY755K\n",
       "\n",
       "[16216 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings_historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|parent_asin|             user_id|\n",
      "+-----------+--------------------+\n",
      "| B01M7UD21X|AFRAYYOGKVOG5B2SG...|\n",
      "| B0BM2J47PC|AE73UZUFL544D3FRO...|\n",
      "| B07MGJZ2MH|AGU7FC2GHGCOSOAYW...|\n",
      "| B07NVL6TJG|AHW5ZMORJKQXUF4BA...|\n",
      "| B07QD9WBGB|AG5BN7XS32ZFCNJT3...|\n",
      "| B07RBYJN37|AGDVSQR73TP57VHWU...|\n",
      "| B08DDK1TP5|AGZQGMHGM7ISG2AP5...|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "data = [\n",
    "    (\"B01M7UD21X\", \"AFRAYYOGKVOG5B2SGEOLZTPLCSXQ\"),\n",
    "    (\"B0BM2J47PC\", \"AE73UZUFL544D3FROI53QWXB5B5A\"),\n",
    "    (\"B07MGJZ2MH\", \"AGU7FC2GHGCOSOAYWM2USUTBPLBA\"),\n",
    "    (\"B07NVL6TJG\", \"AHW5ZMORJKQXUF4BASUKDM3XB6QA\"),\n",
    "    (\"B07QD9WBGB\", \"AG5BN7XS32ZFCNJT3BR6Y43TM3HA\"),\n",
    "    (\"B07RBYJN37\", \"AGDVSQR73TP57VHWUACAIKV5JTGA\"),\n",
    "    (\"B08DDK1TP5\", \"AGZQGMHGM7ISG2AP5J7W5TKBYANQ\")\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"parent_asin\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = session.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B01M7UD21X</td>\n",
       "      <td>AFRAYYOGKVOG5B2SGEOLZTPLCSXQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0BM2J47PC</td>\n",
       "      <td>AE73UZUFL544D3FROI53QWXB5B5A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B07MGJZ2MH</td>\n",
       "      <td>AGU7FC2GHGCOSOAYWM2USUTBPLBA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07NVL6TJG</td>\n",
       "      <td>AHW5ZMORJKQXUF4BASUKDM3XB6QA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B07QD9WBGB</td>\n",
       "      <td>AG5BN7XS32ZFCNJT3BR6Y43TM3HA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B07RBYJN37</td>\n",
       "      <td>AGDVSQR73TP57VHWUACAIKV5JTGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B08DDK1TP5</td>\n",
       "      <td>AGZQGMHGM7ISG2AP5J7W5TKBYANQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  parent_asin                       user_id\n",
       "0  B01M7UD21X  AFRAYYOGKVOG5B2SGEOLZTPLCSXQ\n",
       "1  B0BM2J47PC  AE73UZUFL544D3FROI53QWXB5B5A\n",
       "2  B07MGJZ2MH  AGU7FC2GHGCOSOAYWM2USUTBPLBA\n",
       "3  B07NVL6TJG  AHW5ZMORJKQXUF4BASUKDM3XB6QA\n",
       "4  B07QD9WBGB  AG5BN7XS32ZFCNJT3BR6Y43TM3HA\n",
       "5  B07RBYJN37  AGDVSQR73TP57VHWUACAIKV5JTGA\n",
       "6  B08DDK1TP5  AGZQGMHGM7ISG2AP5J7W5TKBYANQ"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B01M7UD21X</td>\n",
       "      <td>AFRAYYOGKVOG5B2SGEOLZTPLCSXQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0BM2J47PC</td>\n",
       "      <td>AE73UZUFL544D3FROI53QWXB5B5A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B07MGJZ2MH</td>\n",
       "      <td>AGU7FC2GHGCOSOAYWM2USUTBPLBA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07NVL6TJG</td>\n",
       "      <td>AHW5ZMORJKQXUF4BASUKDM3XB6QA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B07QD9WBGB</td>\n",
       "      <td>AG5BN7XS32ZFCNJT3BR6Y43TM3HA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16211</th>\n",
       "      <td>B09TMNNYWH</td>\n",
       "      <td>AGX4UTCIP4SGTHHDA4E3WGPIJFBQ</td>\n",
       "      <td>1.608847e+12</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16212</th>\n",
       "      <td>B07GNX34XV</td>\n",
       "      <td>AH4WBYSLGIAMVLCT2PPRTMNNJD6A</td>\n",
       "      <td>1.552765e+12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16213</th>\n",
       "      <td>B07VXY755K</td>\n",
       "      <td>AHUTNTBX5E65YNIHI2ERA2AMFM4Q</td>\n",
       "      <td>1.575580e+12</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16214</th>\n",
       "      <td>B07VXY755K</td>\n",
       "      <td>AF6RXEUYBSSOMO5RSS23OPB5QKSA</td>\n",
       "      <td>1.656510e+12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16215</th>\n",
       "      <td>B07VXY755K</td>\n",
       "      <td>AHNDJGN3XW2IH2HQFTNSNOKB6NSQ</td>\n",
       "      <td>1.626558e+12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16223 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      parent_asin                       user_id     timestamp  rating\n",
       "0      B01M7UD21X  AFRAYYOGKVOG5B2SGEOLZTPLCSXQ           NaN     NaN\n",
       "1      B0BM2J47PC  AE73UZUFL544D3FROI53QWXB5B5A           NaN     NaN\n",
       "2      B07MGJZ2MH  AGU7FC2GHGCOSOAYWM2USUTBPLBA           NaN     NaN\n",
       "3      B07NVL6TJG  AHW5ZMORJKQXUF4BASUKDM3XB6QA           NaN     NaN\n",
       "4      B07QD9WBGB  AG5BN7XS32ZFCNJT3BR6Y43TM3HA           NaN     NaN\n",
       "...           ...                           ...           ...     ...\n",
       "16211  B09TMNNYWH  AGX4UTCIP4SGTHHDA4E3WGPIJFBQ  1.608847e+12     5.0\n",
       "16212  B07GNX34XV  AH4WBYSLGIAMVLCT2PPRTMNNJD6A  1.552765e+12     1.0\n",
       "16213  B07VXY755K  AHUTNTBX5E65YNIHI2ERA2AMFM4Q  1.575580e+12     5.0\n",
       "16214  B07VXY755K  AF6RXEUYBSSOMO5RSS23OPB5QKSA  1.656510e+12     1.0\n",
       "16215  B07VXY755K  AHNDJGN3XW2IH2HQFTNSNOKB6NSQ  1.626558e+12     1.0\n",
       "\n",
       "[16223 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df.toPandas(), df_ratings_historical])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys-streaming-ml-Mj1TWbkU-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
