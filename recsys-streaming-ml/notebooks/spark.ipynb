{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, struct, array\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, Union, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYSPARK_PYTHON=C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\Scripts\\python.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYSPARK_DRIVER_PYTHON=C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\Scripts\\python.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BROKER_URL = \"kafka0:9093\"\n",
    "RECOMMENDATIONS_TOPIC = \"recommendations\"\n",
    "USER_ACTIONS_TOPIC = \"users.actions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    # load model from checkpoint\n",
    "    import torch    \n",
    "    device = torch.device(\"cuda\")\n",
    "    model = Net().to(device)\n",
    "    checkpoint = load_checkpoint(checkpoint_dir)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    # define predict function in terms of numpy arrays\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        torch_inputs = torch.from_numpy(inputs).to(device)\n",
    "        outputs = model(torch_inputs)\n",
    "        return outputs.cpu().detach().numpy()\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"KafkaRead\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"user_id\", StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER_URL) \\\n",
    "    .option(\"subscribe\", RECOMMENDATIONS_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df = df.selectExpr(\"CAST(value AS STRING) as json_data\") \\\n",
    "                .select(from_json(col(\"json_data\"), schema).alias(\"data\")) \\\n",
    "                .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df.writeStream.format('console').outputMode('append').start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = values_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed = df.selectExpr(\"CAST(value AS STRING) as json_value\") \\\n",
    "    .select(from_json(col(\"json_value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_parsed.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.spark.utils import spark\n",
    "\n",
    "def create_dataframe_from_dict(spark, data):\n",
    "    \"\"\"\n",
    "    Create a DataFrame from a list of dictionaries.\n",
    "    Each dictionary represents a record with a single field `user_id`.\n",
    "    \"\"\"\n",
    "    schema = StructType([StructField(\"user_id\", StringType(), True)])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    return df\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    {\"user_id\": \"A1\"},\n",
    "    {\"user_id\": \"B2\"},\n",
    "    {\"user_id\": \"C3\"},\n",
    "    {\"user_id\": \"D4\"}\n",
    "]\n",
    "\n",
    "# Create Spark session\n",
    "session = spark()\n",
    "\n",
    "# Create DataFrame from data\n",
    "df = create_dataframe_from_dict(session, data)\n",
    "\n",
    "# Show DataFrame\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|     A1|\n",
      "|     B2|\n",
      "|     C3|\n",
      "|     D4|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "def create_item_feature_store(spark):\n",
    "    data = [(i, i) for i in range(100)]\n",
    "    schema = StructType([\n",
    "        StructField(\"parent_asin\", IntegerType(), True),\n",
    "        StructField(\"store_id\", IntegerType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    return df\n",
    "\n",
    "item_feature_store = create_item_feature_store(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|parent_asin|store_id|\n",
      "+-----------+--------+\n",
      "|          0|       0|\n",
      "|          1|       1|\n",
      "|          2|       2|\n",
      "|          3|       3|\n",
      "|          4|       4|\n",
      "|          5|       5|\n",
      "|          6|       6|\n",
      "|          7|       7|\n",
      "|          8|       8|\n",
      "|          9|       9|\n",
      "|         10|      10|\n",
      "|         11|      11|\n",
      "|         12|      12|\n",
      "|         13|      13|\n",
      "|         14|      14|\n",
      "|         15|      15|\n",
      "|         16|      16|\n",
      "|         17|      17|\n",
      "|         18|      18|\n",
      "|         19|      19|\n",
      "+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_feature_store.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MONGO] Connection successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\pandas\\__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from recsys_streaming_ml.db import mongo_db, read_df_from_mongo\n",
    "from recsys_streaming_ml.data.utils import load_feature_maps\n",
    "import random\n",
    "\n",
    "feature_maps = load_feature_maps(\"../.data/feature_maps.pkl\")\n",
    "\n",
    "def read_item_feature_store(db, feature_maps, collection='metadata'):\n",
    "    item_feature_store_raw = read_df_from_mongo(db=db, collection=collection)\n",
    "    item_feature_store = item_feature_store_raw.copy()\n",
    "    item_feature_store['parent_asin'] = item_feature_store['parent_asin'].map(feature_maps['parent_id_map'])\n",
    "    item_feature_store['store_id'] = item_feature_store['store'].map(feature_maps['store_id_map'])\n",
    "    item_feature_store = item_feature_store.drop(columns='store').dropna().astype(int).sort_values(by='parent_asin').reset_index(drop=True)\n",
    "\n",
    "    return item_feature_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_store = read_item_feature_store(mongo_db, feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_store = session.createDataFrame(item_feature_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|parent_asin|store_id|\n",
      "+-----------+--------+\n",
      "|          0|      46|\n",
      "|          1|      74|\n",
      "|          2|      47|\n",
      "|          3|     209|\n",
      "|          4|      48|\n",
      "|          5|     238|\n",
      "|          6|     179|\n",
      "|          7|     239|\n",
      "|          8|      49|\n",
      "|          9|      75|\n",
      "|         10|     132|\n",
      "|         11|     111|\n",
      "|         12|      18|\n",
      "|         13|     112|\n",
      "|         14|       0|\n",
      "|         15|      19|\n",
      "|         16|       1|\n",
      "|         17|     113|\n",
      "|         18|       0|\n",
      "|         19|     240|\n",
      "+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_feature_store.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, create_map, lit, udf\n",
    "from itertools import chain\n",
    "\n",
    "user_id_mapping = {\"A1\": 2, \"B2\": 3, \"C3\": 1, \"D4\": 0}\n",
    "rev_user_id_mapping = {v:k for k,v in user_id_mapping.items()}\n",
    "rev_asin_mapping = {v[0]:f'ID_{v[0]}' for v in item_feature_store.select('parent_asin').distinct().collect()}\n",
    "\n",
    "def process_data(\n",
    "        df: ps.sql.dataframe.DataFrame, \n",
    "        item_feature_store: ps.sql.dataframe.DataFrame, \n",
    "        user_id_mapping: dict[str, int]\n",
    "    ) -> ps.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the DataFrame by mapping user_ids using the provided dictionary.\n",
    "    \"\"\"\n",
    "    mapping_expr = create_map([lit(x) for x in chain(*user_id_mapping.items())])\n",
    "\n",
    "    processed_df = df.withColumn(\"map_user_id\", mapping_expr[col(\"user_id\")])\n",
    "    processed_df = processed_df.crossJoin(item_feature_store)\n",
    "    processed_df = processed_df.select(\"map_user_id\", \"parent_asin\", \"store_id\")\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = process_data(df, item_feature_store, user_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------+\n",
      "|map_user_id|parent_asin|store_id|\n",
      "+-----------+-----------+--------+\n",
      "|          2|          0|       0|\n",
      "|          2|          1|       1|\n",
      "|          2|          2|       2|\n",
      "|          2|          3|       3|\n",
      "|          2|          4|       4|\n",
      "|          2|          5|       5|\n",
      "|          2|          6|       6|\n",
      "|          2|          7|       7|\n",
      "|          2|          8|       8|\n",
      "|          2|          9|       9|\n",
      "|          2|         10|      10|\n",
      "|          2|         11|      11|\n",
      "|          2|         12|      12|\n",
      "|          2|         13|      13|\n",
      "|          2|         14|      14|\n",
      "|          2|         15|      15|\n",
      "|          2|         16|      16|\n",
      "|          2|         17|      17|\n",
      "|          2|         18|      18|\n",
      "|          2|         19|      19|\n",
      "+-----------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, collect_list\n",
    "\n",
    "def get_ranked_topk_predictions(df, k=5):\n",
    "    window = Window.partitionBy(\"map_user_id\").orderBy(col(\"predicted_rating\").desc())\n",
    "\n",
    "    # Add a rank column to rank the rows within each partition by 'sum'\n",
    "    ranked_predictions = df.withColumn(\"rank\", rank().over(window))\n",
    "\n",
    "    # Filter to keep only the top 5 'asin' values for each 'map_user_id'\n",
    "    top_k = ranked_predictions.filter(col(\"rank\") <= k)\n",
    "\n",
    "    return top_k\n",
    "\n",
    "def remap_entities(df, user_id_mapping, asin_mapping):\n",
    "        mapping_expr_user = create_map([lit(x) for x in chain(*user_id_mapping.items())])\n",
    "        mapping_expr_asin = create_map([lit(x) for x in chain(*asin_mapping.items())])\n",
    "\n",
    "        df = df.withColumn(\"user_id\", mapping_expr_user[col(\"map_user_id\")])\n",
    "        df = df.withColumn(\"asin\", mapping_expr_asin[col(\"map_user_id\")])\n",
    "\n",
    "        return df.select(\"user_id\", \"asin\", \"rank\")\n",
    "\n",
    "def list_recommendations(df):\n",
    "    # Aggregate the top k 'asin' values into a list for each 'map_user_id'\n",
    "    result = df.groupBy(\"user_id\").agg(collect_list(\"asin\").alias(\"top_k_asins\"))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_topk = get_ranked_topk_predictions(predictions)\n",
    "remapped_ranked_topk = remap_entities(ranked_topk, rev_user_id_mapping, rev_asin_mapping)\n",
    "recommendation_lists = list_recommendations(remapped_ranked_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_topk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remapped_ranked_topk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_lists.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/path/to/test/data\")\n",
    "preds = df.withColumn(\"preds\", mnist('data')).collect()\n",
    "\n",
    "query = df_parsed.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    #.trigger(processingTime='15 seconds') \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../.data/dataset/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.data.utils import load_feature_maps\n",
    "import random\n",
    "\n",
    "feature_maps = load_feature_maps(\"../.data/feature_maps.pkl\")\n",
    "\n",
    "random_user_ids = random.choices(list(feature_maps['user_id_map'].keys()), k=50)\n",
    "pd.DataFrame(random_user_ids, columns=['user_id']).to_csv(\"../.data/sample_user_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys-streaming-ml-Mj1TWbkU-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
