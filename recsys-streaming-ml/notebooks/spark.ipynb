{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, struct, array\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, Union, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYSPARK_PYTHON=C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\Scripts\\python.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYSPARK_DRIVER_PYTHON=C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\Scripts\\python.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BROKER_URL = \"kafka0:9093\"\n",
    "RECOMMENDATIONS_TOPIC = \"recommendations\"\n",
    "USER_ACTIONS_TOPIC = \"users.actions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    # load model from checkpoint\n",
    "    import torch    \n",
    "    device = torch.device(\"cuda\")\n",
    "    model = Net().to(device)\n",
    "    checkpoint = load_checkpoint(checkpoint_dir)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    # define predict function in terms of numpy arrays\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        torch_inputs = torch.from_numpy(inputs).to(device)\n",
    "        outputs = model(torch_inputs)\n",
    "        return outputs.cpu().detach().numpy()\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"KafkaRead\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"user_id\", StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER_URL) \\\n",
    "    .option(\"subscribe\", RECOMMENDATIONS_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df = df.selectExpr(\"CAST(value AS STRING) as json_data\") \\\n",
    "                .select(from_json(col(\"json_data\"), schema).alias(\"data\")) \\\n",
    "                .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df.writeStream.format('console').outputMode('append').start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = values_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed = df.selectExpr(\"CAST(value AS STRING) as json_value\") \\\n",
    "    .select(from_json(col(\"json_value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_parsed.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.spark.utils import spark\n",
    "\n",
    "def create_dataframe_from_dict(spark, data):\n",
    "    \"\"\"\n",
    "    Create a DataFrame from a list of dictionaries.\n",
    "    Each dictionary represents a record with a single field `user_id`.\n",
    "    \"\"\"\n",
    "    schema = StructType([StructField(\"user_id\", StringType(), True)])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    return df\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    {\"user_id\": \"A1\"},\n",
    "    {\"user_id\": \"B2\"},\n",
    "    {\"user_id\": \"C3\"},\n",
    "    {\"user_id\": \"D4\"}\n",
    "]\n",
    "\n",
    "# Create Spark session\n",
    "session = spark()\n",
    "\n",
    "# Create DataFrame from data\n",
    "df = create_dataframe_from_dict(session, data)\n",
    "\n",
    "# Show DataFrame\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "def create_item_feature_store(spark):\n",
    "    data = [(i, i) for i in range(100)]\n",
    "    schema = StructType([\n",
    "        StructField(\"parent_asin\", IntegerType(), True),\n",
    "        StructField(\"store_id\", IntegerType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    return df\n",
    "\n",
    "item_feature_store = create_item_feature_store(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_store.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.db import mongo_db, read_df_from_mongo\n",
    "from recsys_streaming_ml.data.utils import load_feature_maps\n",
    "import random\n",
    "\n",
    "feature_maps = load_feature_maps(\"../.data/feature_maps.pkl\")\n",
    "\n",
    "def read_item_feature_store(db, feature_maps, collection='metadata'):\n",
    "    item_feature_store_raw = read_df_from_mongo(db=db, collection=collection)\n",
    "    item_feature_store = item_feature_store_raw.copy()\n",
    "    item_feature_store['parent_asin'] = item_feature_store['parent_asin'].map(feature_maps['parent_id_map'])\n",
    "    item_feature_store['store_id'] = item_feature_store['store'].map(feature_maps['store_id_map'])\n",
    "    item_feature_store = item_feature_store.drop(columns='store').dropna().astype(int).sort_values(by='parent_asin').reset_index(drop=True)\n",
    "\n",
    "    return item_feature_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_store = read_item_feature_store(mongo_db, feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_store = session.createDataFrame(item_feature_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_store.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, create_map, lit, udf\n",
    "from itertools import chain\n",
    "\n",
    "user_id_mapping = {\"A1\": 2, \"B2\": 3, \"C3\": 1, \"D4\": 0}\n",
    "rev_user_id_mapping = {v:k for k,v in user_id_mapping.items()}\n",
    "rev_asin_mapping = {v[0]:f'ID_{v[0]}' for v in item_feature_store.select('parent_asin').distinct().collect()}\n",
    "\n",
    "def process_data(\n",
    "        df: ps.sql.dataframe.DataFrame, \n",
    "        item_feature_store: ps.sql.dataframe.DataFrame, \n",
    "        user_id_mapping: dict[str, int]\n",
    "    ) -> ps.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the DataFrame by mapping user_ids using the provided dictionary.\n",
    "    \"\"\"\n",
    "    mapping_expr = create_map([lit(x) for x in chain(*user_id_mapping.items())])\n",
    "\n",
    "    processed_df = df.withColumn(\"map_user_id\", mapping_expr[col(\"user_id\")])\n",
    "    processed_df = processed_df.crossJoin(item_feature_store)\n",
    "    processed_df = processed_df.select(\"map_user_id\", \"parent_asin\", \"store_id\")\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = process_data(df, item_feature_store, user_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, collect_list\n",
    "\n",
    "def get_ranked_topk_predictions(df, k=5):\n",
    "    window = Window.partitionBy(\"map_user_id\").orderBy(col(\"predicted_rating\").desc())\n",
    "\n",
    "    # Add a rank column to rank the rows within each partition by 'sum'\n",
    "    ranked_predictions = df.withColumn(\"rank\", rank().over(window))\n",
    "\n",
    "    # Filter to keep only the top 5 'asin' values for each 'map_user_id'\n",
    "    top_k = ranked_predictions.filter(col(\"rank\") <= k)\n",
    "\n",
    "    return top_k\n",
    "\n",
    "def remap_entities(df, user_id_mapping, asin_mapping):\n",
    "        mapping_expr_user = create_map([lit(x) for x in chain(*user_id_mapping.items())])\n",
    "        mapping_expr_asin = create_map([lit(x) for x in chain(*asin_mapping.items())])\n",
    "\n",
    "        df = df.withColumn(\"user_id\", mapping_expr_user[col(\"map_user_id\")])\n",
    "        df = df.withColumn(\"asin\", mapping_expr_asin[col(\"map_user_id\")])\n",
    "\n",
    "        return df.select(\"user_id\", \"asin\", \"rank\")\n",
    "\n",
    "def list_recommendations(df):\n",
    "    # Aggregate the top k 'asin' values into a list for each 'map_user_id'\n",
    "    result = df.groupBy(\"user_id\").agg(collect_list(\"asin\").alias(\"top_k_asins\"))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_topk = get_ranked_topk_predictions(predictions)\n",
    "remapped_ranked_topk = remap_entities(ranked_topk, rev_user_id_mapping, rev_asin_mapping)\n",
    "recommendation_lists = list_recommendations(remapped_ranked_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_topk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remapped_ranked_topk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_lists.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/path/to/test/data\")\n",
    "preds = df.withColumn(\"preds\", mnist('data')).collect()\n",
    "\n",
    "query = df_parsed.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    #.trigger(processingTime='15 seconds') \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../.data/dataset/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.data.utils import load_feature_maps\n",
    "import random\n",
    "\n",
    "feature_maps = load_feature_maps(\"../.data/feature_maps.pkl\")\n",
    "\n",
    "random_user_ids = random.choices(list(feature_maps['user_id_map'].keys()), k=50)\n",
    "pd.DataFrame(random_user_ids, columns=['user_id']).to_csv(\"../.data/sample_user_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "#model = torch.jit.load(\"C:/Users/Milosz/Projects/recsys-streaming/recsys-streaming-ml/.runs/DeepFM/2024-05-11_15-29-20/model.pt\", map_location='cpu')\n",
    "model_input = torch.randint(0, 40, (1, 3)).to(torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepFM(emb_dim=8, hidden_dim=[32, 24, 10], feature_sizes=[100, 100, 100])\n",
    "model.eval()\n",
    "model_input = torch.randint(0, 40, (1, 3)).to(torch.float)\n",
    "traced_script_module = torch.jit.trace(model, model_input)\n",
    "\n",
    "# Save the TorchScript model\n",
    "traced_script_module.save(\"../.model_repository/DeepFM/1/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.model.model import DeepFM\n",
    "# model = torch.load(\"C:/Users/Milosz/Projects/recsys-streaming/recsys-streaming-ml/.runs/DeepFM/2024-05-11_15-29-20/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepFM(\n",
       "  (V): EmbeddingNet(\n",
       "    (embeddings): ModuleDict(\n",
       "      (0): Embedding(100, 8)\n",
       "      (1): Embedding(100, 8)\n",
       "      (2): Embedding(100, 8)\n",
       "    )\n",
       "  )\n",
       "  (fm): FM()\n",
       "  (dnn): MLP(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=24, out_features=32, bias=True)\n",
       "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=32, out_features=24, bias=True)\n",
       "      (5): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): Dropout(p=0.1, inplace=False)\n",
       "      (8): Linear(in_features=24, out_features=10, bias=True)\n",
       "      (9): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): Dropout(p=0.1, inplace=False)\n",
       "      (12): Linear(in_features=10, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../.model_repository/DeepFM/1/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, model_input, \"../.model_repository/DeepFM/1/model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, struct, col, array\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, Union, Dict, StructType, StructField, DataType\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "from functools import partial\n",
    "\n",
    "from recsys_streaming_ml.spark.utils import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "session = spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               input|\n",
      "+--------------------+\n",
      "|[0.8619455, 0.431...|\n",
      "|[0.18523002, 0.31...|\n",
      "|[0.49610206, 0.59...|\n",
      "|[0.04170201, 0.68...|\n",
      "|[0.8219223, 0.164...|\n",
      "|[0.17303495, 0.22...|\n",
      "|[0.67281836, 0.80...|\n",
      "|[0.42195797, 0.51...|\n",
      "|[0.03965295, 0.75...|\n",
      "|[0.27317488, 0.67...|\n",
      "|[0.19653319, 0.85...|\n",
      "|[0.90088624, 0.74...|\n",
      "|[0.5246734, 0.251...|\n",
      "|[0.31020227, 0.56...|\n",
      "|[0.69740117, 0.87...|\n",
      "|[0.6936094, 0.151...|\n",
      "|[0.6217257, 0.591...|\n",
      "|[0.81691366, 0.27...|\n",
      "|[0.38775757, 0.60...|\n",
      "|[0.41836286, 0.24...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the PySpark DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"input\", ArrayType(FloatType()), False)\n",
    "])\n",
    "\n",
    "data = [(np.random.rand(3).astype(np.float32).tolist(),) for _ in range(100)]\n",
    "df = session.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\n[UNSUPPORTED_DATATYPE] Unsupported data type \"DATATYPE\".(line 1, pos 11)\n\n== SQL ==\noutput_col DataType\n-----------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m: outputs})\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Register UDF with Spark\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m evaluate_model_udf \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_col DataType\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctionType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPandasUDFType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCALAR\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:395\u001b[0m, in \u001b[0;36mpandas_udf\u001b[1;34m(f, returnType, functionType)\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mpartial(_create_pandas_udf, returnType\u001b[38;5;241m=\u001b[39mreturn_type, evalType\u001b[38;5;241m=\u001b[39meval_type)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_pandas_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:480\u001b[0m, in \u001b[0;36m_create_pandas_udf\u001b[1;34m(f, returnType, evalType)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _create_connect_udf(f, returnType, evalType)\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalType\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\udf.py:84\u001b[0m, in \u001b[0;36m_create_udf\u001b[1;34m(f, returnType, evalType, name, deterministic)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Set the name of the UserDefinedFunction object to be the name of function f\u001b[39;00m\n\u001b[0;32m     81\u001b[0m udf_obj \u001b[38;5;241m=\u001b[39m UserDefinedFunction(\n\u001b[0;32m     82\u001b[0m     f, returnType\u001b[38;5;241m=\u001b[39mreturnType, name\u001b[38;5;241m=\u001b[39mname, evalType\u001b[38;5;241m=\u001b[39mevalType, deterministic\u001b[38;5;241m=\u001b[39mdeterministic\n\u001b[0;32m     83\u001b[0m )\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mudf_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\udf.py:433\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    426\u001b[0m wrapper\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\n\u001b[0;32m    430\u001b[0m )\n\u001b[0;32m    432\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 433\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mreturnType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturnType\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    434\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mevalType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    435\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mdeterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\udf.py:214\u001b[0m, in \u001b[0;36mUserDefinedFunction.returnType\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returnType_placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returnType\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returnType_placeholder \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_datatype_string\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_returnType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType \u001b[38;5;241m==\u001b[39m PythonEvalType\u001b[38;5;241m.\u001b[39mSQL_SCALAR_PANDAS_UDF\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType \u001b[38;5;241m==\u001b[39m PythonEvalType\u001b[38;5;241m.\u001b[39mSQL_SCALAR_PANDAS_ITER_UDF\n\u001b[0;32m    219\u001b[0m ):\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\types.py:1319\u001b[0m, in \u001b[0;36m_parse_datatype_string\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m from_ddl_datatype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstruct<\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m s\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m-> 1319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\types.py:1309\u001b[0m, in \u001b[0;36m_parse_datatype_string\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_datatype_json_string(\n\u001b[0;32m   1302\u001b[0m         cast(JVMView, sc\u001b[38;5;241m.\u001b[39m_jvm)\n\u001b[0;32m   1303\u001b[0m         \u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mpython\u001b[38;5;241m.\u001b[39mPythonSQLUtils\u001b[38;5;241m.\u001b[39mparseDataType(type_str)\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m   1305\u001b[0m     )\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1308\u001b[0m     \u001b[38;5;66;03m# DDL format, \"fieldname datatype, fieldname datatype\".\u001b[39;00m\n\u001b[1;32m-> 1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_ddl_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1312\u001b[0m         \u001b[38;5;66;03m# For backwards compatibility, \"integer\", \"struct<fieldname: datatype>\" and etc.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\types.py:1297\u001b[0m, in \u001b[0;36m_parse_datatype_string.<locals>.from_ddl_schema\u001b[1;34m(type_str)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_ddl_schema\u001b[39m(type_str: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataType:\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_datatype_json_string(\n\u001b[1;32m-> 1297\u001b[0m         \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJVMView\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromDDL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_str\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m   1298\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mParseException\u001b[0m: \n[UNSUPPORTED_DATATYPE] Unsupported data type \"DATATYPE\".(line 1, pos 11)\n\n== SQL ==\noutput_col DataType\n-----------^^^\n"
     ]
    }
   ],
   "source": [
    "# Triton client setup\n",
    "TRITON_URL = 'localhost:8000'\n",
    "MODEL_NAME = 'DeepFM'\n",
    "\n",
    "def evaluate_model(batch_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    inputs = batch_df['input'].to_list()\n",
    "    \n",
    "    triton_client = httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "    \n",
    "    input_tensor = httpclient.InferInput('input', inputs[0].shape, 'FP32')\n",
    "    input_tensor.set_data_from_numpy(np.array(inputs))\n",
    "    \n",
    "    outputs = []\n",
    "    for input_data in inputs:\n",
    "        input_tensor.set_data_from_numpy(input_data)\n",
    "        result = triton_client.infer(model_name=MODEL_NAME, inputs=[input_tensor])\n",
    "        output_data = result.as_numpy('output')\n",
    "        outputs.append(output_data)\n",
    "    \n",
    "    return pd.DataFrame({'output': outputs})\n",
    "\n",
    "# Register UDF with Spark\n",
    "evaluate_model_udf = pandas_udf(evaluate_model, returnType='output_col DataType', functionType=PandasUDFType.SCALAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRITON_GRPC_URL = 'localhost:8001'\n",
    "MODEL_NAME = 'DeepFM'\n",
    "\n",
    "\n",
    "def triton_fn(triton_uri, model_name):\n",
    "    import numpy as np\n",
    "    import tritonclient.grpc as grpcclient\n",
    "    \n",
    "    np_types = {\n",
    "      \"BOOL\": np.dtype(np.bool8),\n",
    "      \"INT8\": np.dtype(np.int8),\n",
    "      \"INT16\": np.dtype(np.int16),\n",
    "      \"INT32\": np.dtype(np.int32),\n",
    "      \"INT64\": np.dtype(np.int64),\n",
    "      \"FP16\": np.dtype(np.float16),\n",
    "      \"FP32\": np.dtype(np.float32),\n",
    "      \"FP64\": np.dtype(np.float64),\n",
    "      \"FP64\": np.dtype(np.double),\n",
    "      \"BYTES\": np.dtype(object)\n",
    "    }\n",
    "\n",
    "    client = grpcclient.InferenceServerClient(triton_uri)\n",
    "    model_meta = client.get_model_metadata(model_name)\n",
    "    \n",
    "    def predict(inputs):\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            # single ndarray input\n",
    "            request = [grpcclient.InferInput(model_meta.inputs[0].name, inputs.shape, model_meta.inputs[0].datatype)]\n",
    "            request[0].set_data_from_numpy(inputs.astype(np_types[model_meta.inputs[0].datatype]))\n",
    "        else:\n",
    "            # dict of multiple ndarray inputs\n",
    "            request = [grpcclient.InferInput(i.name, inputs[i.name].shape, i.datatype) for i in model_meta.inputs]\n",
    "            for i in request:\n",
    "                i.set_data_from_numpy(inputs[i.name()].astype(np_types[i.datatype()]))\n",
    "        \n",
    "        response = client.infer(model_name, inputs=request)\n",
    "        \n",
    "        if len(model_meta.outputs) > 1:\n",
    "            # return dictionary of numpy arrays\n",
    "            return {o.name: response.as_numpy(o.name) for o in model_meta.outputs}\n",
    "        else:\n",
    "            # return single numpy array\n",
    "            return response.as_numpy(model_meta.outputs[0].name)\n",
    "        \n",
    "    return predict\n",
    "\n",
    "recommender = predict_batch_udf(partial(triton_fn, triton_uri=TRITON_GRPC_URL, model_name=MODEL_NAME),\n",
    "                          input_tensor_shapes=[[3]],\n",
    "                          return_type=ArrayType(FloatType()),\n",
    "                          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"c:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\ml\\functions.py\", line 765, in predict\n    predict_fn = make_predict_fn()\n  File \"C:\\Users\\Milosz\\AppData\\Local\\Temp\\ipykernel_26864\\857280881.py\", line 23, in triton_fn\n  File \"C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\tritonclient\\grpc\\_client.py\", line 522, in get_model_metadata\n    raise_error_grpc(rpc_error)\n  File \"C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\tritonclient\\grpc\\_utils.py\", line 77, in raise_error_grpc\n    raise get_error_grpc(rpc_error) from None\ntritonclient.utils.InferenceServerException: [StatusCode.UNAVAILABLE] Request for unknown model: 'DeepFM' is not found\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m results_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m, recommender(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Show results\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mresults_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    960\u001b[0m     )\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"c:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\ml\\functions.py\", line 765, in predict\n    predict_fn = make_predict_fn()\n  File \"C:\\Users\\Milosz\\AppData\\Local\\Temp\\ipykernel_26864\\857280881.py\", line 23, in triton_fn\n  File \"C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\tritonclient\\grpc\\_client.py\", line 522, in get_model_metadata\n    raise_error_grpc(rpc_error)\n  File \"C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\tritonclient\\grpc\\_utils.py\", line 77, in raise_error_grpc\n    raise get_error_grpc(rpc_error) from None\ntritonclient.utils.InferenceServerException: [StatusCode.UNAVAILABLE] Request for unknown model: 'DeepFM' is not found\n"
     ]
    }
   ],
   "source": [
    "results_df = df.withColumn('output', recommender(df['input']))\n",
    "\n",
    "# Show results\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply UDF to DataFrame\n",
    "results_df = df.withColumn('output', evaluate_model_udf(df['input']))\n",
    "\n",
    "# Show results\n",
    "results_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys-streaming-ml-Mj1TWbkU-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
