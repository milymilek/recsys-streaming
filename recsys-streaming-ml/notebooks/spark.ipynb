{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, struct, array\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, Union, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYSPARK_PYTHON=C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\Scripts\\python.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYSPARK_DRIVER_PYTHON=C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\Scripts\\python.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BROKER_URL = \"kafka0:9093\"\n",
    "RECOMMENDATIONS_TOPIC = \"recommendations\"\n",
    "USER_ACTIONS_TOPIC = \"users.actions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    # load model from checkpoint\n",
    "    import torch    \n",
    "    device = torch.device(\"cuda\")\n",
    "    model = Net().to(device)\n",
    "    checkpoint = load_checkpoint(checkpoint_dir)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    # define predict function in terms of numpy arrays\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        torch_inputs = torch.from_numpy(inputs).to(device)\n",
    "        outputs = model(torch_inputs)\n",
    "        return outputs.cpu().detach().numpy()\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"KafkaRead\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"user_id\", StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER_URL) \\\n",
    "    .option(\"subscribe\", RECOMMENDATIONS_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df = df.selectExpr(\"CAST(value AS STRING) as json_data\") \\\n",
    "                .select(from_json(col(\"json_data\"), schema).alias(\"data\")) \\\n",
    "                .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df.writeStream.format('console').outputMode('append').start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = values_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed = df.selectExpr(\"CAST(value AS STRING) as json_value\") \\\n",
    "    .select(from_json(col(\"json_value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_parsed.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.spark.utils import spark\n",
    "\n",
    "def create_dataframe_from_dict(spark, data):\n",
    "    \"\"\"\n",
    "    Create a DataFrame from a list of dictionaries.\n",
    "    Each dictionary represents a record with a single field `user_id`.\n",
    "    \"\"\"\n",
    "    schema = StructType([StructField(\"user_id\", StringType(), True)])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    return df\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    {\"user_id\": \"A1\"},\n",
    "    {\"user_id\": \"B2\"},\n",
    "    {\"user_id\": \"C3\"},\n",
    "    {\"user_id\": \"D4\"}\n",
    "]\n",
    "\n",
    "# Create Spark session\n",
    "session = spark()\n",
    "\n",
    "# Create DataFrame from data\n",
    "df = create_dataframe_from_dict(session, data)\n",
    "\n",
    "# Show DataFrame\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "def create_item_feature_store(spark):\n",
    "    data = [(i, i) for i in range(100)]\n",
    "    schema = StructType([\n",
    "        StructField(\"parent_asin\", IntegerType(), True),\n",
    "        StructField(\"store_id\", IntegerType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    return df\n",
    "\n",
    "item_feature_store = create_item_feature_store(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_store.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.db import mongo_db, read_df_from_mongo\n",
    "from recsys_streaming_ml.data.utils import load_feature_maps\n",
    "import random\n",
    "\n",
    "feature_maps = load_feature_maps(\"../.data/feature_maps.pkl\")\n",
    "\n",
    "def read_item_feature_store(db, feature_maps, collection='metadata'):\n",
    "    item_feature_store_raw = read_df_from_mongo(db=db, collection=collection)\n",
    "    item_feature_store = item_feature_store_raw.copy()\n",
    "    item_feature_store['parent_asin'] = item_feature_store['parent_asin'].map(feature_maps['parent_id_map'])\n",
    "    item_feature_store['store_id'] = item_feature_store['store'].map(feature_maps['store_id_map'])\n",
    "    item_feature_store = item_feature_store.drop(columns='store').dropna().astype(int).sort_values(by='parent_asin').reset_index(drop=True)\n",
    "\n",
    "    return item_feature_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_store = read_item_feature_store(mongo_db, feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_store = session.createDataFrame(item_feature_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_store.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, create_map, lit, udf\n",
    "from itertools import chain\n",
    "\n",
    "user_id_mapping = {\"A1\": 2, \"B2\": 3, \"C3\": 1, \"D4\": 0}\n",
    "rev_user_id_mapping = {v:k for k,v in user_id_mapping.items()}\n",
    "rev_asin_mapping = {v[0]:f'ID_{v[0]}' for v in item_feature_store.select('parent_asin').distinct().collect()}\n",
    "\n",
    "def process_data(\n",
    "        df: ps.sql.dataframe.DataFrame, \n",
    "        item_feature_store: ps.sql.dataframe.DataFrame, \n",
    "        user_id_mapping: dict[str, int]\n",
    "    ) -> ps.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the DataFrame by mapping user_ids using the provided dictionary.\n",
    "    \"\"\"\n",
    "    mapping_expr = create_map([lit(x) for x in chain(*user_id_mapping.items())])\n",
    "\n",
    "    processed_df = df.withColumn(\"map_user_id\", mapping_expr[col(\"user_id\")])\n",
    "    processed_df = processed_df.crossJoin(item_feature_store)\n",
    "    processed_df = processed_df.select(\"map_user_id\", \"parent_asin\", \"store_id\")\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = process_data(df, item_feature_store, user_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, collect_list\n",
    "\n",
    "def get_ranked_topk_predictions(df, k=5):\n",
    "    window = Window.partitionBy(\"map_user_id\").orderBy(col(\"predicted_rating\").desc())\n",
    "\n",
    "    # Add a rank column to rank the rows within each partition by 'sum'\n",
    "    ranked_predictions = df.withColumn(\"rank\", rank().over(window))\n",
    "\n",
    "    # Filter to keep only the top 5 'asin' values for each 'map_user_id'\n",
    "    top_k = ranked_predictions.filter(col(\"rank\") <= k)\n",
    "\n",
    "    return top_k\n",
    "\n",
    "def remap_entities(df, user_id_mapping, asin_mapping):\n",
    "        mapping_expr_user = create_map([lit(x) for x in chain(*user_id_mapping.items())])\n",
    "        mapping_expr_asin = create_map([lit(x) for x in chain(*asin_mapping.items())])\n",
    "\n",
    "        df = df.withColumn(\"user_id\", mapping_expr_user[col(\"map_user_id\")])\n",
    "        df = df.withColumn(\"asin\", mapping_expr_asin[col(\"map_user_id\")])\n",
    "\n",
    "        return df.select(\"user_id\", \"asin\", \"rank\")\n",
    "\n",
    "def list_recommendations(df):\n",
    "    # Aggregate the top k 'asin' values into a list for each 'map_user_id'\n",
    "    result = df.groupBy(\"user_id\").agg(collect_list(\"asin\").alias(\"top_k_asins\"))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_topk = get_ranked_topk_predictions(predictions)\n",
    "remapped_ranked_topk = remap_entities(ranked_topk, rev_user_id_mapping, rev_asin_mapping)\n",
    "recommendation_lists = list_recommendations(remapped_ranked_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_topk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remapped_ranked_topk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_lists.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/path/to/test/data\")\n",
    "preds = df.withColumn(\"preds\", mnist('data')).collect()\n",
    "\n",
    "query = df_parsed.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    #.trigger(processingTime='15 seconds') \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../.data/dataset/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.data.utils import load_feature_maps\n",
    "import random\n",
    "\n",
    "feature_maps = load_feature_maps(\"../.data/feature_maps.pkl\")\n",
    "\n",
    "random_user_ids = random.choices(list(feature_maps['user_id_map'].keys()), k=50)\n",
    "pd.DataFrame(random_user_ids, columns=['user_id']).to_csv(\"../.data/sample_user_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "#model = torch.jit.load(\"C:/Users/Milosz/Projects/recsys-streaming/recsys-streaming-ml/.runs/DeepFM/2024-05-11_15-29-20/model.pt\", map_location='cpu')\n",
    "model_input = torch.randint(0, 40, (1, 3)).to(torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepFM(emb_dim=8, hidden_dim=[32, 24, 10], feature_sizes=[100, 100, 100])\n",
    "model.eval()\n",
    "model_input = torch.randint(0, 40, (1, 3)).to(torch.float)\n",
    "traced_script_module = torch.jit.trace(model, model_input)\n",
    "\n",
    "# Save the TorchScript model\n",
    "traced_script_module.save(\"../.model_repository/DeepFM/1/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.model.model import DeepFM\n",
    "# model = torch.load(\"C:/Users/Milosz/Projects/recsys-streaming/recsys-streaming-ml/.runs/DeepFM/2024-05-11_15-29-20/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepFM(\n",
       "  (V): EmbeddingNet(\n",
       "    (embeddings): ModuleDict(\n",
       "      (0): Embedding(100, 8)\n",
       "      (1): Embedding(100, 8)\n",
       "      (2): Embedding(100, 8)\n",
       "    )\n",
       "  )\n",
       "  (fm): FM()\n",
       "  (dnn): MLP(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=24, out_features=32, bias=True)\n",
       "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=32, out_features=24, bias=True)\n",
       "      (5): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): Dropout(p=0.1, inplace=False)\n",
       "      (8): Linear(in_features=24, out_features=10, bias=True)\n",
       "      (9): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): Dropout(p=0.1, inplace=False)\n",
       "      (12): Linear(in_features=10, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../.model_repository/DeepFM/1/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, model_input, \"../.model_repository/DeepFM/1/model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, struct, col, array\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, Union, Dict, StructType, StructField, DataType\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "from functools import partial\n",
    "\n",
    "from recsys_streaming_ml.spark.utils import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "session = spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               input|\n",
      "+--------------------+\n",
      "|[0.8619455, 0.431...|\n",
      "|[0.18523002, 0.31...|\n",
      "|[0.49610206, 0.59...|\n",
      "|[0.04170201, 0.68...|\n",
      "|[0.8219223, 0.164...|\n",
      "|[0.17303495, 0.22...|\n",
      "|[0.67281836, 0.80...|\n",
      "|[0.42195797, 0.51...|\n",
      "|[0.03965295, 0.75...|\n",
      "|[0.27317488, 0.67...|\n",
      "|[0.19653319, 0.85...|\n",
      "|[0.90088624, 0.74...|\n",
      "|[0.5246734, 0.251...|\n",
      "|[0.31020227, 0.56...|\n",
      "|[0.69740117, 0.87...|\n",
      "|[0.6936094, 0.151...|\n",
      "|[0.6217257, 0.591...|\n",
      "|[0.81691366, 0.27...|\n",
      "|[0.38775757, 0.60...|\n",
      "|[0.41836286, 0.24...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the PySpark DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"input\", ArrayType(FloatType()), False)\n",
    "])\n",
    "\n",
    "data = [(np.random.rand(3).astype(np.float32).tolist(),) for _ in range(100)]\n",
    "df = session.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\n[UNSUPPORTED_DATATYPE] Unsupported data type \"DATATYPE\".(line 1, pos 11)\n\n== SQL ==\noutput_col DataType\n-----------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m: outputs})\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Register UDF with Spark\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m evaluate_model_udf \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_col DataType\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctionType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPandasUDFType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCALAR\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:395\u001b[0m, in \u001b[0;36mpandas_udf\u001b[1;34m(f, returnType, functionType)\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mpartial(_create_pandas_udf, returnType\u001b[38;5;241m=\u001b[39mreturn_type, evalType\u001b[38;5;241m=\u001b[39meval_type)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_pandas_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:480\u001b[0m, in \u001b[0;36m_create_pandas_udf\u001b[1;34m(f, returnType, evalType)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _create_connect_udf(f, returnType, evalType)\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalType\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\udf.py:84\u001b[0m, in \u001b[0;36m_create_udf\u001b[1;34m(f, returnType, evalType, name, deterministic)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Set the name of the UserDefinedFunction object to be the name of function f\u001b[39;00m\n\u001b[0;32m     81\u001b[0m udf_obj \u001b[38;5;241m=\u001b[39m UserDefinedFunction(\n\u001b[0;32m     82\u001b[0m     f, returnType\u001b[38;5;241m=\u001b[39mreturnType, name\u001b[38;5;241m=\u001b[39mname, evalType\u001b[38;5;241m=\u001b[39mevalType, deterministic\u001b[38;5;241m=\u001b[39mdeterministic\n\u001b[0;32m     83\u001b[0m )\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mudf_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\udf.py:433\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    426\u001b[0m wrapper\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\n\u001b[0;32m    430\u001b[0m )\n\u001b[0;32m    432\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 433\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mreturnType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturnType\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    434\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mevalType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    435\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mdeterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\udf.py:214\u001b[0m, in \u001b[0;36mUserDefinedFunction.returnType\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returnType_placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returnType\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returnType_placeholder \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_datatype_string\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_returnType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType \u001b[38;5;241m==\u001b[39m PythonEvalType\u001b[38;5;241m.\u001b[39mSQL_SCALAR_PANDAS_UDF\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType \u001b[38;5;241m==\u001b[39m PythonEvalType\u001b[38;5;241m.\u001b[39mSQL_SCALAR_PANDAS_ITER_UDF\n\u001b[0;32m    219\u001b[0m ):\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\types.py:1319\u001b[0m, in \u001b[0;36m_parse_datatype_string\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m from_ddl_datatype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstruct<\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m s\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m-> 1319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\types.py:1309\u001b[0m, in \u001b[0;36m_parse_datatype_string\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_datatype_json_string(\n\u001b[0;32m   1302\u001b[0m         cast(JVMView, sc\u001b[38;5;241m.\u001b[39m_jvm)\n\u001b[0;32m   1303\u001b[0m         \u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mpython\u001b[38;5;241m.\u001b[39mPythonSQLUtils\u001b[38;5;241m.\u001b[39mparseDataType(type_str)\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m   1305\u001b[0m     )\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1308\u001b[0m     \u001b[38;5;66;03m# DDL format, \"fieldname datatype, fieldname datatype\".\u001b[39;00m\n\u001b[1;32m-> 1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_ddl_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1312\u001b[0m         \u001b[38;5;66;03m# For backwards compatibility, \"integer\", \"struct<fieldname: datatype>\" and etc.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\types.py:1297\u001b[0m, in \u001b[0;36m_parse_datatype_string.<locals>.from_ddl_schema\u001b[1;34m(type_str)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_ddl_schema\u001b[39m(type_str: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataType:\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_datatype_json_string(\n\u001b[1;32m-> 1297\u001b[0m         \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJVMView\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromDDL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_str\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m   1298\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mParseException\u001b[0m: \n[UNSUPPORTED_DATATYPE] Unsupported data type \"DATATYPE\".(line 1, pos 11)\n\n== SQL ==\noutput_col DataType\n-----------^^^\n"
     ]
    }
   ],
   "source": [
    "# Triton client setup\n",
    "TRITON_URL = 'localhost:8000'\n",
    "MODEL_NAME = 'DeepFM'\n",
    "\n",
    "def evaluate_model(batch_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    inputs = batch_df['input'].to_list()\n",
    "    \n",
    "    triton_client = httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "    \n",
    "    input_tensor = httpclient.InferInput('input', inputs[0].shape, 'FP32')\n",
    "    input_tensor.set_data_from_numpy(np.array(inputs))\n",
    "    \n",
    "    outputs = []\n",
    "    for input_data in inputs:\n",
    "        input_tensor.set_data_from_numpy(input_data)\n",
    "        result = triton_client.infer(model_name=MODEL_NAME, inputs=[input_tensor])\n",
    "        output_data = result.as_numpy('output')\n",
    "        outputs.append(output_data)\n",
    "    \n",
    "    return pd.DataFrame({'output': outputs})\n",
    "\n",
    "# Register UDF with Spark\n",
    "evaluate_model_udf = pandas_udf(evaluate_model, returnType='output_col DataType', functionType=PandasUDFType.SCALAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRITON_GRPC_URL = 'localhost:8001'\n",
    "MODEL_NAME = 'DeepFM'\n",
    "\n",
    "\n",
    "def triton_fn(triton_uri, model_name):\n",
    "    import numpy as np\n",
    "    import tritonclient.grpc as grpcclient\n",
    "    \n",
    "    np_types = {\n",
    "      \"BOOL\": np.dtype(np.bool8),\n",
    "      \"INT8\": np.dtype(np.int8),\n",
    "      \"INT16\": np.dtype(np.int16),\n",
    "      \"INT32\": np.dtype(np.int32),\n",
    "      \"INT64\": np.dtype(np.int64),\n",
    "      \"FP16\": np.dtype(np.float16),\n",
    "      \"FP32\": np.dtype(np.float32),\n",
    "      \"FP64\": np.dtype(np.float64),\n",
    "      \"FP64\": np.dtype(np.double),\n",
    "      \"BYTES\": np.dtype(object)\n",
    "    }\n",
    "\n",
    "    client = grpcclient.InferenceServerClient(triton_uri)\n",
    "    model_meta = client.get_model_metadata(model_name)\n",
    "    \n",
    "    def predict(inputs):\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            # single ndarray input\n",
    "            request = [grpcclient.InferInput(model_meta.inputs[0].name, inputs.shape, model_meta.inputs[0].datatype)]\n",
    "            request[0].set_data_from_numpy(inputs.astype(np_types[model_meta.inputs[0].datatype]))\n",
    "        else:\n",
    "            # dict of multiple ndarray inputs\n",
    "            request = [grpcclient.InferInput(i.name, inputs[i.name].shape, i.datatype) for i in model_meta.inputs]\n",
    "            for i in request:\n",
    "                i.set_data_from_numpy(inputs[i.name()].astype(np_types[i.datatype()]))\n",
    "        \n",
    "        response = client.infer(model_name, inputs=request)\n",
    "        \n",
    "        if len(model_meta.outputs) > 1:\n",
    "            # return dictionary of numpy arrays\n",
    "            return {o.name: response.as_numpy(o.name) for o in model_meta.outputs}\n",
    "        else:\n",
    "            # return single numpy array\n",
    "            return response.as_numpy(model_meta.outputs[0].name)\n",
    "        \n",
    "    return predict\n",
    "\n",
    "recommender = predict_batch_udf(partial(triton_fn, triton_uri=TRITON_GRPC_URL, model_name=MODEL_NAME),\n",
    "                          input_tensor_shapes=[[3]],\n",
    "                          return_type=ArrayType(FloatType()),\n",
    "                          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"c:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\ml\\functions.py\", line 765, in predict\n    predict_fn = make_predict_fn()\n  File \"C:\\Users\\Milosz\\AppData\\Local\\Temp\\ipykernel_26864\\857280881.py\", line 23, in triton_fn\n  File \"C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\tritonclient\\grpc\\_client.py\", line 522, in get_model_metadata\n    raise_error_grpc(rpc_error)\n  File \"C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\tritonclient\\grpc\\_utils.py\", line 77, in raise_error_grpc\n    raise get_error_grpc(rpc_error) from None\ntritonclient.utils.InferenceServerException: [StatusCode.UNAVAILABLE] Request for unknown model: 'DeepFM' is not found\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m results_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m, recommender(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Show results\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mresults_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    960\u001b[0m     )\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"c:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\ml\\functions.py\", line 765, in predict\n    predict_fn = make_predict_fn()\n  File \"C:\\Users\\Milosz\\AppData\\Local\\Temp\\ipykernel_26864\\857280881.py\", line 23, in triton_fn\n  File \"C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\tritonclient\\grpc\\_client.py\", line 522, in get_model_metadata\n    raise_error_grpc(rpc_error)\n  File \"C:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\tritonclient\\grpc\\_utils.py\", line 77, in raise_error_grpc\n    raise get_error_grpc(rpc_error) from None\ntritonclient.utils.InferenceServerException: [StatusCode.UNAVAILABLE] Request for unknown model: 'DeepFM' is not found\n"
     ]
    }
   ],
   "source": [
    "results_df = df.withColumn('output', recommender(df['input']))\n",
    "\n",
    "# Show results\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "sys.path.append(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DISABLE_SPARK_ENVS'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.spark.utils import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys-streaming-ml\\.data\\dataset\\dataset_spark.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:/Users/Milosz/Projects/recsys-streaming/recsys-streaming-ml/.data/dataset/dataset_spark.csv/part-00000-b5b5d215-6187-49ea-905e-70202bd20248-c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = session.read.option(\"header\", \"true\").csv(\"C:/Users/Milosz/Projects/recsys-streaming/recsys-streaming-ml/.data/dataset/dataset_spark.csv/part-00000-b5b5d215-6187-49ea-905e-70202bd20248-c000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o57.csv.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:180)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:133)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:68)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:539)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:405)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/Milosz/Projects/recsys-streaming/recsys-streaming-ml/.data/dataset/dataset_spark.csv/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\sql\\readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Milosz\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recsys-streaming-ml-Mj1TWbkU-py3.10\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o57.csv.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:180)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:162)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:133)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:68)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:539)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:405)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n"
     ]
    }
   ],
   "source": [
    "spark_df = session.read.option(\"header\", \"true\").csv(\"C:/Users/Milosz/Projects/recsys-streaming/recsys-streaming-ml/.data/dataset/dataset_spark.csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../.data/dataset'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"../\" + DATASET_FILE.as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.config import DATASET_FILE\n",
    "import os\n",
    "def find_csv_file(directory_path):\n",
    "    # Iterate over the files in the directory\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            # Check if the file has a .csv suffix\n",
    "            if file.endswith('.csv'):\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(find_csv_file(\"../\" + DATASET_FILE.as_posix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.data.utils import load_feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = load_feature_maps('../.data/feature_maps.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['user_id_map', 'parent_id_map', 'store_id_map'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8029      0\n",
       "8030      0\n",
       "8031      0\n",
       "8032      0\n",
       "8028      0\n",
       "       ... \n",
       "9355    376\n",
       "9354    376\n",
       "9353    376\n",
       "9359    376\n",
       "9335    376\n",
       "Name: store, Length: 16201, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.store.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Crunchy Mama Box': 0,\n",
       " 'Mechaly': 1,\n",
       " 'CLUBTAC': 2,\n",
       " 'The Dapper Dog Box': 3,\n",
       " 'Coffee Beanery': 4,\n",
       " 'Transcends Monthly Crystal Subscription': 5,\n",
       " 'Mysteries in Time - adventures through history': 6,\n",
       " 'Pusheen': 7,\n",
       " 'Bitsbox': 8,\n",
       " 'La Fraise Boutique Adalie': 9,\n",
       " 'Beauteque': 10,\n",
       " 'meowbox': 11,\n",
       " 'MadrasIn': 12,\n",
       " \"Well Read's Box of Books\": 13,\n",
       " 'MindfulSouls': 14,\n",
       " 'VAHDAM': 15,\n",
       " 'Geek Fuel EXP': 16,\n",
       " 'Tinkering Toddler Crates': 17,\n",
       " 'Autsy Box': 18,\n",
       " 'Dayspa Body Basics': 19,\n",
       " 'Bath Blessing Box': 20,\n",
       " 'Avelas': 21,\n",
       " 'RICK AND MORTY': 22,\n",
       " 'SHIMAJIRO': 23,\n",
       " 'Nature Gnaws': 24,\n",
       " 'My Coffee and Book Club': 25,\n",
       " 'KaffeeScape': 26,\n",
       " 'goScribbler': 27,\n",
       " 'Boujee Brown': 28,\n",
       " 'Skivvie NIX': 29,\n",
       " 'LiveGlam Inc.': 30,\n",
       " 'Pressed': 31,\n",
       " 'SOCK FANCY': 32,\n",
       " 'Inked, LLC': 33,\n",
       " 'Snoqualmie Falls Candy Shoppe': 34,\n",
       " 'Enchanted Crystal': 35,\n",
       " 'Loved Again Media': 36,\n",
       " 'GetFaithbox': 37,\n",
       " 'Chakra Box': 38,\n",
       " 'Aroma Thyme': 39,\n",
       " 'Trendy Apparel Shop': 40,\n",
       " 'Loot Crate, Inc.': 41,\n",
       " 'Driftaway Coffee': 42,\n",
       " 'Frito Lay': 43,\n",
       " 'Foot Cardigan': 44,\n",
       " 'PupJoy': 45,\n",
       " 'Altman Plants': 46,\n",
       " 'My First Reading Club': 47,\n",
       " 'Sock Panda': 48,\n",
       " 'Bubbles&Books': 49,\n",
       " \"America's Test Kitchen Kids\": 50,\n",
       " 'Rays of Light': 51,\n",
       " \"ERIKA'S TEA ROOM MEMORIES MADE WITH EVERY CUP!\": 52,\n",
       " 'River Colony Trading': 53,\n",
       " 'Smartass & Sass': 54,\n",
       " 'Animal Jam Box': 55,\n",
       " 'Cross Box Kits': 56,\n",
       " 'Vnus Box': 57,\n",
       " 'Star Trek': 58,\n",
       " 'Pub Shirt Club': 59,\n",
       " 'Arctify Marvel': 60,\n",
       " 'Primal Kitchen': 61,\n",
       " 'Jeep': 62,\n",
       " 'PupMomCrate': 63,\n",
       " 'EmbraceBox': 64,\n",
       " 'Groovy Lab in a Box': 65,\n",
       " 'BOLDBOX': 66,\n",
       " 'STAR WARS': 67,\n",
       " 'Fat Brain Toys': 68,\n",
       " 'SoKawaii': 69,\n",
       " 'Urban You for Me': 70,\n",
       " 'Love Aiki': 71,\n",
       " 'Buffalo Bills': 72,\n",
       " 'SnackSack': 73,\n",
       " 'South Park': 74,\n",
       " 'Confetti Dash': 75,\n",
       " 'Play Learn Repeat': 76,\n",
       " 'Guitar Crate': 77,\n",
       " 'Puzzle Culture': 78,\n",
       " 'Watch Gang': 79,\n",
       " 'SockFly': 80,\n",
       " 'Hippie Dippie': 81,\n",
       " 'PriceMeNow': 82,\n",
       " 'Reveal Book Box': 83,\n",
       " 'Asurion': 84,\n",
       " 'Bombs and Bubbles Box': 85,\n",
       " 'The GEMS Camp': 86,\n",
       " 'Ignite English': 87,\n",
       " 'Avatar The Last Airbender': 88,\n",
       " 'Makeblock': 89,\n",
       " 'Kevia Style, LLC': 90,\n",
       " 'PHILOSOCKPHY': 91,\n",
       " 'Little Red Kitchen Bake Shop': 92,\n",
       " 'Just Glow': 93,\n",
       " 'Tea Runners': 94,\n",
       " 'Supernow': 95,\n",
       " 'Barbie': 96,\n",
       " 'Lash Stuff Simply Exquisite': 97,\n",
       " 'Nickelodeon': 98,\n",
       " 'Our Aroma': 99,\n",
       " 'Cotton Cuts LLC LIMITED LIABILITY COMPANY MISSOURI': 100,\n",
       " 'Brick Loot': 101,\n",
       " 'MFRC Wish Box': 102,\n",
       " 'Lovers Fresh Baked': 103,\n",
       " 'Fantasy Monthly': 104,\n",
       " 'The Expanse': 105,\n",
       " 'Stranger Things': 106,\n",
       " 'SnackNation': 107,\n",
       " 'Snoopy': 108,\n",
       " 'Love Goodly': 109,\n",
       " 'Green Kid Crafts': 110,\n",
       " 'KNITCRATE': 111,\n",
       " 'teawrks': 112,\n",
       " 'The Ring Boxes': 113,\n",
       " 'Inspire Me Korea Ltd': 114,\n",
       " 'Irish at Heart': 115,\n",
       " 'The Comic Garage': 116,\n",
       " 'eat2explore': 117,\n",
       " 'The FitBoxx': 118,\n",
       " 'Juice From the RAW': 119,\n",
       " 'The Legend of Vox Machina': 120,\n",
       " 'Hot Wheels': 121,\n",
       " 'My Thrill Club': 122,\n",
       " 'Psychic envy': 123,\n",
       " 'STEM Club': 124,\n",
       " 'Grateful Bread Company LLC': 125,\n",
       " 'LOLJerky': 126,\n",
       " 'Coastal Crafts DIY': 127,\n",
       " 'JoJo Siwa': 128,\n",
       " 'Zero Pack': 129,\n",
       " 'Holstee': 130,\n",
       " 'Simple Loose Leaf Tea Company': 131,\n",
       " 'Costa Farms': 132,\n",
       " 'X Workbox': 133,\n",
       " 'The Paisley Box': 134,\n",
       " \"Annie's\": 135,\n",
       " 'Comic Book Subs': 136,\n",
       " 'PINK PEWTER': 137,\n",
       " '123 BABY BOX': 138,\n",
       " 'Amazon Music': 139,\n",
       " 'OT For Me': 140,\n",
       " 'Boost Your BBQ': 141,\n",
       " 'Drag Society': 142,\n",
       " 'GQ Best Stuff': 143,\n",
       " 'Space & Beyond Box': 144,\n",
       " 'SONAGE': 145,\n",
       " 'Fullscale Subscription Box': 146,\n",
       " 'THE FABULOUS PLANNER': 147,\n",
       " 'Smalltown Table': 148,\n",
       " 'WWE': 149,\n",
       " 'Muse Illuminate Store': 150,\n",
       " 'Loot Crate': 151,\n",
       " 'SpiceBreeze': 152,\n",
       " 'Allure Beauty Box': 153,\n",
       " 'Jurassic World': 154,\n",
       " '50 States Of Mine': 155,\n",
       " 'Elevate Snack Box': 156,\n",
       " 'Succulent Studios': 157,\n",
       " 'Mainstreet Flower Market': 158,\n",
       " 'TinkerTots Boxes': 159,\n",
       " 'Chasin Unicorns': 160,\n",
       " 'Neverland Books': 161,\n",
       " 'PPC Store': 162,\n",
       " 'GlobeIn': 163,\n",
       " 'Match Made Coffee': 164,\n",
       " 'KiwiGrub': 165,\n",
       " 'Silverquicken': 166,\n",
       " 'Cairn': 167,\n",
       " 'Tomorrows Laundry': 168,\n",
       " 'Speach Family Candy Shoppe, Inc': 169,\n",
       " 'COCOTIQUE': 170,\n",
       " 'Clio Coffee Inc': 171,\n",
       " 'SPREZZA': 172,\n",
       " 'Southern Skeins Yarn': 173,\n",
       " 'STEM Discovery Boxes': 174,\n",
       " 'amazon music': 175,\n",
       " 'Cooper & Kid': 176,\n",
       " 'Dungeons & Dragons': 177,\n",
       " 'BloomsyBox': 178,\n",
       " 'BoxDog': 179,\n",
       " 'Highlights for Children': 180,\n",
       " 'The Kids Craft': 181,\n",
       " 'Czech Beads Exclusive': 182,\n",
       " 'Field to Cup': 183,\n",
       " 'Mini Mitts': 184,\n",
       " 'The Office': 185,\n",
       " 'My Hero Crate': 186,\n",
       " 'Wonder Crate': 187,\n",
       " 'How to Be A Redhead': 188,\n",
       " 'No-Gi Surprise': 189,\n",
       " 'CratedWithLove': 190,\n",
       " 'Artshine Inc': 191,\n",
       " 'Downtown Pet Supply': 192,\n",
       " 'The Russia Box': 193,\n",
       " 'Active Kids': 194,\n",
       " 'The Care Crate Co.': 195,\n",
       " 'Carnivore Club': 196,\n",
       " 'SPONGEBOB SQUAREPANTS': 197,\n",
       " 'Gogh Box Art Crate': 198,\n",
       " 'Le Panier Francais': 199,\n",
       " 'Adults & Crafts': 200,\n",
       " 'The Herb Collective': 201,\n",
       " 'ChaskiBox': 202,\n",
       " 'Jerky Subscription': 203,\n",
       " 'KIDS READ DAILY': 204,\n",
       " 'Funko': 205,\n",
       " 'Raddish': 206,\n",
       " 'Lady Box': 207,\n",
       " 'ESSENCE SKINCARE': 208,\n",
       " 'Skylar': 209,\n",
       " 'Bean Box': 210,\n",
       " 'Perfect Samplers': 211,\n",
       " 'The Dally Grind': 212,\n",
       " 'iBbeautiful': 213,\n",
       " 'Crush Crate': 214,\n",
       " 'DreamCutFlowers': 215,\n",
       " 'ELLA Inspires Magazine': 216,\n",
       " 'The Storytime Box': 217,\n",
       " 'Foodbevy': 218,\n",
       " 'BATTLBOX': 219,\n",
       " 'Preschool2U': 220,\n",
       " 'Hunt A Killer': 221,\n",
       " 'The Pink Sugar Box': 222,\n",
       " 'Rael, Inc': 223,\n",
       " 'hotspot pets': 224,\n",
       " 'Snack Mountain': 225,\n",
       " 'Basic Man': 226,\n",
       " 'Marvel': 227,\n",
       " 'Quilty Box': 228,\n",
       " 'Pawstruck': 229,\n",
       " 'E Squared Sweets LLC': 230,\n",
       " 'La Tea Dah': 231,\n",
       " 'MunchPak': 232,\n",
       " 'Feeling Fab Box': 233,\n",
       " 'SUCCULENTS BOX': 234,\n",
       " 'Gainz Box': 235,\n",
       " 'Korean Snack Box': 236,\n",
       " 'YVO - Your Very Own': 237,\n",
       " 'TheraBox': 238,\n",
       " \"Nelson's Tea\": 239,\n",
       " 'Wag Healthy Club': 240,\n",
       " 'Little Dreamers Club': 241,\n",
       " 'Disney': 242,\n",
       " 'LUXSB': 243,\n",
       " 'My Toothbrush Club': 244,\n",
       " 'Sovereign Code': 245,\n",
       " 'CatLadyBox': 246,\n",
       " 'RescueBox': 247,\n",
       " 'Think Outside Boxes': 248,\n",
       " 'Jerky Snob': 249,\n",
       " 'Miraculous': 250,\n",
       " 'Beauty By Francesca': 251,\n",
       " 'BabyFaceDiary': 252,\n",
       " 'BUNNY  JAMES .': 253,\n",
       " 'DateBox Club': 254,\n",
       " 'Eyescream Beauty': 255,\n",
       " 'Wickbox': 256,\n",
       " 'Your Fam Box': 257,\n",
       " 'L.O.L. Surprise!': 258,\n",
       " \"Rollin' n Bowlin'\": 259,\n",
       " 'Kidstir': 260,\n",
       " 'Cajun Crate': 261,\n",
       " 'We Craft Box': 262,\n",
       " 'LOONEY TUNES': 263,\n",
       " 'DC Comics': 264,\n",
       " 'Tea Sparrow': 265,\n",
       " 'Date Night In Box': 266,\n",
       " 'Dose of Literature': 267,\n",
       " 'IScape Candle Co.': 268,\n",
       " 'HARRY POTTER': 269,\n",
       " 'Mel': 270,\n",
       " 'KOS Coffee': 271,\n",
       " 'Toy Box Monthly': 272,\n",
       " 'My Christmas Crate': 273,\n",
       " 'Bokksu': 274,\n",
       " \"Medusa's Make-Up\": 275,\n",
       " 'Baketivity': 276,\n",
       " 'CultureFly': 277,\n",
       " 'SANRIO': 278,\n",
       " 'Crystalized Sparkle Box': 279,\n",
       " 'iMagicNails': 280,\n",
       " 'Magic The Gathering': 281,\n",
       " 'Home Made Luxe Craft Kits': 282,\n",
       " 'Platterful': 283,\n",
       " 'Madrasin': 284,\n",
       " 'breobox': 285,\n",
       " 'JAPANCRATE': 286,\n",
       " 'MMIZOO': 287,\n",
       " 'ArtSnacks': 288,\n",
       " 'The Keto Box': 289,\n",
       " 'Dungeon in a Box': 290,\n",
       " 'ATLG': 291,\n",
       " 'Kids American Monthly Subscription Box': 292,\n",
       " 'Mullybox': 293,\n",
       " 'Coloring And Classics': 294,\n",
       " 'Vegancuts': 295,\n",
       " 'MTV': 296,\n",
       " 'Junior Chef Box': 297,\n",
       " 'Fit Snack Box': 298,\n",
       " 'Sips By': 299,\n",
       " 'Tribe Beauty Box': 300,\n",
       " 'Masters of the Universe: Revelation': 301,\n",
       " 'OHM Beads': 302,\n",
       " 'bravbox': 303,\n",
       " 'June Moon Spice Company': 304,\n",
       " 'For The Faithful': 305,\n",
       " 'Bold Box': 306,\n",
       " 'Trendy Butler': 307,\n",
       " 'ParaBox Monthly': 308,\n",
       " 'Comicrealm': 309,\n",
       " 'Seoul Box': 310,\n",
       " 'MALEBASICS': 311,\n",
       " 'Barkbox': 312,\n",
       " 'FACETORY': 313,\n",
       " 'Ortega': 314,\n",
       " 'Learn With Mochi': 315,\n",
       " 'EnjoyLeggings': 316,\n",
       " 'Treats': 317,\n",
       " 'The Beatles': 318,\n",
       " 'Orgain': 319,\n",
       " 'The Winston Box': 320,\n",
       " 'Lip Monthly': 321,\n",
       " 'The Beard Club': 322,\n",
       " 'Disney Pixar': 323,\n",
       " 'OjO Games and Toys': 324,\n",
       " 'Simply Created': 325,\n",
       " 'Culture Fly': 326,\n",
       " 'Bravado Music Icons': 327,\n",
       " 'DAYFEN': 328,\n",
       " 'Sprezza': 329,\n",
       " 'Tipsy Pixie Tees': 330,\n",
       " 'Paperkitty': 331,\n",
       " 'Candy Club': 332,\n",
       " 'DolliBu': 333,\n",
       " 'Classic Candy Box': 334,\n",
       " 'BARKBOX': 335,\n",
       " 'Naruto Shippuden': 336,\n",
       " 'Flaire': 337,\n",
       " 'POUTBOX': 338,\n",
       " 'Cotton Cuts': 339,\n",
       " 'BARK IF YOU WANT SOME': 340,\n",
       " 'ColombianCoffeeUS': 341,\n",
       " \"Annie's Crafts\": 342,\n",
       " 'badboxx': 343,\n",
       " 'FetchBox': 344,\n",
       " 'Explore By Mail': 345,\n",
       " 'Toynk': 346,\n",
       " 'Herb + Stone Apothecary': 347,\n",
       " 'American Plant Exchange': 348,\n",
       " 'Cora Crea Crafts': 349,\n",
       " 'TROVE': 350,\n",
       " 'Prime Accessories HQ': 351,\n",
       " 'Enjoy Flowers': 352,\n",
       " 'Hamptons Box Club': 353,\n",
       " 'KIWI ECO BOX': 354,\n",
       " 'Cosmopolitan': 355,\n",
       " 'Little Passports': 356,\n",
       " 'Heavenly Candle': 357,\n",
       " 'Sandvik Publishing': 358,\n",
       " 'EXCELIUS': 359,\n",
       " 'KitNipBox': 360,\n",
       " 'Essence Of Argan': 361,\n",
       " 'noblo': 362,\n",
       " 'JARMABOX': 363,\n",
       " 'LivingBetter50': 364,\n",
       " 'Sticker Savages': 365,\n",
       " 'Hawthorne House': 366,\n",
       " 'The Killer Coffee Co': 367,\n",
       " 'TOKYOTREAT': 368,\n",
       " 'Tonies': 369,\n",
       " 'TeaFavors': 370,\n",
       " 'IntrovertsRetreat': 371,\n",
       " 'Fearless Nail Art': 372,\n",
       " 'Akibento': 373,\n",
       " 'Pokmon': 374,\n",
       " 'STRONG self(ie)': 375,\n",
       " \"Gentleman's Box\": 376}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps['store_id_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        264\n",
       "1        264\n",
       "2        264\n",
       "3        264\n",
       "4        264\n",
       "        ... \n",
       "16196     44\n",
       "16197     44\n",
       "16198     44\n",
       "16199    102\n",
       "16200    264\n",
       "Name: store, Length: 16201, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+-------+-----+\n",
      "|parent_asin|    timestamp|rating|user_id|store|\n",
      "+-----------+-------------+------+-------+-----+\n",
      "|          0|1639408671038|   2.0|   6551|  264|\n",
      "|          0|1670094428392|   5.0|   3191|  264|\n",
      "|          0|1653138455274|   5.0|  14957|  264|\n",
      "|          0|1636644871638|   5.0|   8711|  264|\n",
      "|          0|1639006187872|   5.0|   7138|  264|\n",
      "|          0|1652340580030|   3.0|  12961|  264|\n",
      "|          0|1633974025922|   5.0|   9479|  264|\n",
      "|          0|1649532524800|   5.0|      0|  264|\n",
      "|          0|1635107916926|   3.0|   3433|  264|\n",
      "|          0|1634350509767|   5.0|   6782|  264|\n",
      "|          0|1635187736470|   4.0|    298|  264|\n",
      "|          0|1633827953703|   5.0|   2841|  264|\n",
      "|          0|1666670589458|   5.0|  14892|  264|\n",
      "|          0|1648242079936|   5.0|   1184|  264|\n",
      "|          0|1640290884841|   5.0|  10483|  264|\n",
      "|          0|1633800053883|   5.0|   3583|  264|\n",
      "|          1|1565731866942|   1.0|  14233|   10|\n",
      "|          1|1594956895069|   5.0|  11200|   10|\n",
      "|          1|1624322553032|   4.0|  11697|   10|\n",
      "|          1|1602677834804|   5.0|   2696|   10|\n",
      "+-----------+-------------+------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_streaming_ml.db import read_df_from_mongo, mongo_db\n",
    "df_ratings_historical = read_df_from_mongo(db=mongo_db, collection='ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rating</th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1602133857705</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AEMJ2EG5ODOCYUTI54NBXZHDJGSQ</td>\n",
       "      <td>B09WC47S3V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1609110735433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>AEEJBFZKUBEEMBZUZJV4UHFVEEBQ</td>\n",
       "      <td>B07QL1JRCN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1609937315319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AGSVZNZBTSGQBKZDZTQYEZHGDPCQ</td>\n",
       "      <td>B08N5QKX1Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1616156351887</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AFDERNB6BIR3U2DOR3S2KX7KJJXQ</td>\n",
       "      <td>B07KM6T8GV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1559533206066</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AE6P2YJ6FKX332MD56GPJFSHXNJQ</td>\n",
       "      <td>B07NVKNVNM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16211</th>\n",
       "      <td>1608846627043</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AGX4UTCIP4SGTHHDA4E3WGPIJFBQ</td>\n",
       "      <td>B09TMNNYWH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16212</th>\n",
       "      <td>1552764727421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AH4WBYSLGIAMVLCT2PPRTMNNJD6A</td>\n",
       "      <td>B07GNX34XV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16213</th>\n",
       "      <td>1575580196049</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AHUTNTBX5E65YNIHI2ERA2AMFM4Q</td>\n",
       "      <td>B07VXY755K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16214</th>\n",
       "      <td>1656510163413</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AF6RXEUYBSSOMO5RSS23OPB5QKSA</td>\n",
       "      <td>B07VXY755K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16215</th>\n",
       "      <td>1626557904080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AHNDJGN3XW2IH2HQFTNSNOKB6NSQ</td>\n",
       "      <td>B07VXY755K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16216 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           timestamp  rating                       user_id parent_asin\n",
       "0      1602133857705     1.0  AEMJ2EG5ODOCYUTI54NBXZHDJGSQ  B09WC47S3V\n",
       "1      1609110735433     2.0  AEEJBFZKUBEEMBZUZJV4UHFVEEBQ  B07QL1JRCN\n",
       "2      1609937315319     1.0  AGSVZNZBTSGQBKZDZTQYEZHGDPCQ  B08N5QKX1Y\n",
       "3      1616156351887     5.0  AFDERNB6BIR3U2DOR3S2KX7KJJXQ  B07KM6T8GV\n",
       "4      1559533206066     5.0  AE6P2YJ6FKX332MD56GPJFSHXNJQ  B07NVKNVNM\n",
       "...              ...     ...                           ...         ...\n",
       "16211  1608846627043     5.0  AGX4UTCIP4SGTHHDA4E3WGPIJFBQ  B09TMNNYWH\n",
       "16212  1552764727421     1.0  AH4WBYSLGIAMVLCT2PPRTMNNJD6A  B07GNX34XV\n",
       "16213  1575580196049     5.0  AHUTNTBX5E65YNIHI2ERA2AMFM4Q  B07VXY755K\n",
       "16214  1656510163413     1.0  AF6RXEUYBSSOMO5RSS23OPB5QKSA  B07VXY755K\n",
       "16215  1626557904080     1.0  AHNDJGN3XW2IH2HQFTNSNOKB6NSQ  B07VXY755K\n",
       "\n",
       "[16216 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings_historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|parent_asin|             user_id|\n",
      "+-----------+--------------------+\n",
      "| B01M7UD21X|AFRAYYOGKVOG5B2SG...|\n",
      "| B0BM2J47PC|AE73UZUFL544D3FRO...|\n",
      "| B07MGJZ2MH|AGU7FC2GHGCOSOAYW...|\n",
      "| B07NVL6TJG|AHW5ZMORJKQXUF4BA...|\n",
      "| B07QD9WBGB|AG5BN7XS32ZFCNJT3...|\n",
      "| B07RBYJN37|AGDVSQR73TP57VHWU...|\n",
      "| B08DDK1TP5|AGZQGMHGM7ISG2AP5...|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "data = [\n",
    "    (\"B01M7UD21X\", \"AFRAYYOGKVOG5B2SGEOLZTPLCSXQ\"),\n",
    "    (\"B0BM2J47PC\", \"AE73UZUFL544D3FROI53QWXB5B5A\"),\n",
    "    (\"B07MGJZ2MH\", \"AGU7FC2GHGCOSOAYWM2USUTBPLBA\"),\n",
    "    (\"B07NVL6TJG\", \"AHW5ZMORJKQXUF4BASUKDM3XB6QA\"),\n",
    "    (\"B07QD9WBGB\", \"AG5BN7XS32ZFCNJT3BR6Y43TM3HA\"),\n",
    "    (\"B07RBYJN37\", \"AGDVSQR73TP57VHWUACAIKV5JTGA\"),\n",
    "    (\"B08DDK1TP5\", \"AGZQGMHGM7ISG2AP5J7W5TKBYANQ\")\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"parent_asin\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = session.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B01M7UD21X</td>\n",
       "      <td>AFRAYYOGKVOG5B2SGEOLZTPLCSXQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0BM2J47PC</td>\n",
       "      <td>AE73UZUFL544D3FROI53QWXB5B5A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B07MGJZ2MH</td>\n",
       "      <td>AGU7FC2GHGCOSOAYWM2USUTBPLBA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07NVL6TJG</td>\n",
       "      <td>AHW5ZMORJKQXUF4BASUKDM3XB6QA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B07QD9WBGB</td>\n",
       "      <td>AG5BN7XS32ZFCNJT3BR6Y43TM3HA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B07RBYJN37</td>\n",
       "      <td>AGDVSQR73TP57VHWUACAIKV5JTGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B08DDK1TP5</td>\n",
       "      <td>AGZQGMHGM7ISG2AP5J7W5TKBYANQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  parent_asin                       user_id\n",
       "0  B01M7UD21X  AFRAYYOGKVOG5B2SGEOLZTPLCSXQ\n",
       "1  B0BM2J47PC  AE73UZUFL544D3FROI53QWXB5B5A\n",
       "2  B07MGJZ2MH  AGU7FC2GHGCOSOAYWM2USUTBPLBA\n",
       "3  B07NVL6TJG  AHW5ZMORJKQXUF4BASUKDM3XB6QA\n",
       "4  B07QD9WBGB  AG5BN7XS32ZFCNJT3BR6Y43TM3HA\n",
       "5  B07RBYJN37  AGDVSQR73TP57VHWUACAIKV5JTGA\n",
       "6  B08DDK1TP5  AGZQGMHGM7ISG2AP5J7W5TKBYANQ"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B01M7UD21X</td>\n",
       "      <td>AFRAYYOGKVOG5B2SGEOLZTPLCSXQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0BM2J47PC</td>\n",
       "      <td>AE73UZUFL544D3FROI53QWXB5B5A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B07MGJZ2MH</td>\n",
       "      <td>AGU7FC2GHGCOSOAYWM2USUTBPLBA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07NVL6TJG</td>\n",
       "      <td>AHW5ZMORJKQXUF4BASUKDM3XB6QA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B07QD9WBGB</td>\n",
       "      <td>AG5BN7XS32ZFCNJT3BR6Y43TM3HA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16211</th>\n",
       "      <td>B09TMNNYWH</td>\n",
       "      <td>AGX4UTCIP4SGTHHDA4E3WGPIJFBQ</td>\n",
       "      <td>1.608847e+12</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16212</th>\n",
       "      <td>B07GNX34XV</td>\n",
       "      <td>AH4WBYSLGIAMVLCT2PPRTMNNJD6A</td>\n",
       "      <td>1.552765e+12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16213</th>\n",
       "      <td>B07VXY755K</td>\n",
       "      <td>AHUTNTBX5E65YNIHI2ERA2AMFM4Q</td>\n",
       "      <td>1.575580e+12</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16214</th>\n",
       "      <td>B07VXY755K</td>\n",
       "      <td>AF6RXEUYBSSOMO5RSS23OPB5QKSA</td>\n",
       "      <td>1.656510e+12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16215</th>\n",
       "      <td>B07VXY755K</td>\n",
       "      <td>AHNDJGN3XW2IH2HQFTNSNOKB6NSQ</td>\n",
       "      <td>1.626558e+12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16223 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      parent_asin                       user_id     timestamp  rating\n",
       "0      B01M7UD21X  AFRAYYOGKVOG5B2SGEOLZTPLCSXQ           NaN     NaN\n",
       "1      B0BM2J47PC  AE73UZUFL544D3FROI53QWXB5B5A           NaN     NaN\n",
       "2      B07MGJZ2MH  AGU7FC2GHGCOSOAYWM2USUTBPLBA           NaN     NaN\n",
       "3      B07NVL6TJG  AHW5ZMORJKQXUF4BASUKDM3XB6QA           NaN     NaN\n",
       "4      B07QD9WBGB  AG5BN7XS32ZFCNJT3BR6Y43TM3HA           NaN     NaN\n",
       "...           ...                           ...           ...     ...\n",
       "16211  B09TMNNYWH  AGX4UTCIP4SGTHHDA4E3WGPIJFBQ  1.608847e+12     5.0\n",
       "16212  B07GNX34XV  AH4WBYSLGIAMVLCT2PPRTMNNJD6A  1.552765e+12     1.0\n",
       "16213  B07VXY755K  AHUTNTBX5E65YNIHI2ERA2AMFM4Q  1.575580e+12     5.0\n",
       "16214  B07VXY755K  AF6RXEUYBSSOMO5RSS23OPB5QKSA  1.656510e+12     1.0\n",
       "16215  B07VXY755K  AHNDJGN3XW2IH2HQFTNSNOKB6NSQ  1.626558e+12     1.0\n",
       "\n",
       "[16223 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df.toPandas(), df_ratings_historical])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys-streaming-ml-Mj1TWbkU-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
